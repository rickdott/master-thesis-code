{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find transformations with base model that maximize the difference between prediction and shuffled prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import netCDF4\n",
    "import xarray as xr\n",
    "from pathlib import Path\n",
    "from hmpai.pytorch.models import *\n",
    "from hmpai.training import split_data_on_participants, split_participants, split_participants_into_folds\n",
    "from hmpai.pytorch.training import train, validate, calculate_class_weights, train_and_test, k_fold_cross_validate, test, calculate_global_class_weights\n",
    "from hmpai.pytorch.utilities import DEVICE, set_global_seed, get_summary_str, save_model, load_model\n",
    "from hmpai.pytorch.generators import SAT1Dataset, MultiXArrayDataset, MultiXArrayProbaDataset\n",
    "from hmpai.data import SAT1_STAGES_ACCURACY, SAT_CLASSES_ACCURACY\n",
    "from hmpai.visualization import plot_confusion_matrix\n",
    "from hmpai.pytorch.normalization import *\n",
    "from torchinfo import summary\n",
    "from hmpai.utilities import print_results, CHANNELS_2D, AR_SAT1_CHANNELS\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose\n",
    "from hmpai.pytorch.transforms import *\n",
    "from collections import defaultdict\n",
    "from hmpai.pytorch.mamba import *\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import json\n",
    "from hmpai.pytorch.utilities import save_tensor\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DATA_PATH = Path(os.getenv(\"DATA_PATH\"))\n",
    "models = []\n",
    "N_FOLDS = 6\n",
    "labels = SAT_CLASSES_ACCURACY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_path = Path(\"../../logs/transformation_validation\")\n",
    "files = list(logs_path.glob('*/*.csv'))\n",
    "dataframes = []\n",
    "\n",
    "for file in files:\n",
    "    data = pd.read_csv(file)\n",
    "    data['transform'] = file.parts[-2] # Get transform name\n",
    "    data['fold'] = int(re.search('[0-9]*\\.', file.parts[-1])[0][:-1]) # Get fold index\n",
    "    dataframes.append(data)\n",
    "\n",
    "data = pd.concat(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "uq_transforms = data[~data['transform'].str.contains('-')]['transform'].unique()\n",
    "legend_labels = {''.join([char for char in tf if char.isupper() or char == '-']): tf  for tf in uq_transforms}\n",
    "legend_labels = [f\"{abbr}: {name}\" for abbr, name in legend_labels.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['(pred, true)'] = data[['0', '1', '2', '3']].mean(axis=1)\n",
    "data['(shuffled, true)'] = data[['4', '5', '6', '7']].mean(axis=1)\n",
    "data['pred_max'] = data[['8', '9', '10', '11']].mean(axis=1)\n",
    "data['shuffled_max'] = data[['12', '13', '14', '15']].mean(axis=1)\n",
    "\n",
    "data['transform'] = data['transform'].apply(lambda x: ''.join([char for char in x if char.isupper() or char == '-']))\n",
    "data_melted = pd.melt(data, id_vars=['transform'], \n",
    "                      value_vars=['(pred, true)', '(shuffled, true)'],\n",
    "                      var_name='Type', value_name='Average')\n",
    "\n",
    "data_melted_max = pd.melt(data, id_vars=['transform'], \n",
    "                      value_vars=['pred_max', 'shuffled_max'],\n",
    "                      var_name='Type', value_name='Average')\n",
    "mean_values = data.groupby(['transform'])[['(pred, true)', '(shuffled, true)']].mean()\n",
    "mean_values['difference'] = mean_values['(pred, true)'] - mean_values['(shuffled, true)']\n",
    "ordered_transforms = mean_values.sort_values('difference', ascending=False).index\n",
    "\n",
    "legend_text = \"\\n\".join(legend_labels)\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, sharex=True, figsize=(len(ordered_transforms), 6))\n",
    "# plt.figure(figsize=(18, 6))\n",
    "# plt.figtext(0.9, 0.5, legend_text, fontsize=12, verticalalignment='center')\n",
    "\n",
    "# TODO: Plot differences over this graph somehow\n",
    "ax[0].set_ylabel('Kullback-Leibler divergence')\n",
    "ax[0].set_ylim((0,5))\n",
    "sns.violinplot(x='transform', y='Average', hue='Type', data=data_melted, palette='mako_r',\n",
    "               split=True, cut=0, order=ordered_transforms, ax=ax[0])\n",
    "\n",
    "sns.violinplot(x='transform', y='Average', hue='Type', data=data_melted_max, palette='mako_r',\n",
    "               split=True, cut=0, order=ordered_transforms, ax=ax[1])\n",
    "ax[1].set_ylim(0, 0.1)\n",
    "ax[1].set_ylabel('Average probability peak height')\n",
    "ax[1].set_xlabel('Transform')\n",
    "plt.xticks(rotation=45)\n",
    "# plt.ylim(-0.001, 0.001)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define transform configurations, separately for train and testval, probably dont want most transforms in case of testval but should still be configurable\n",
    "\n",
    "Can probably create dataset per fold, then set transform by doing `train_data.transform=transform`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First test if transforms by themselves make a difference, assume additive behaviour\n",
    "transforms = [\n",
    "    (None, None),\n",
    "    (Compose([StartJitterTransform(62, 1.0)]), None),\n",
    "    (Compose([EndJitterTransform(63, 1.0)]), None),\n",
    "    (Compose([ReverseTimeTransform()]), None),\n",
    "    (Compose([GaussianNoise()]), None),\n",
    "    (Compose([TimeMaskTransform()]), None),\n",
    "    (Compose([TimeDropoutTransform()]), None),\n",
    "    (Compose([ChannelsDropout()]), None),\n",
    "    (Compose([ConcatenateTransform(0.5)]), None),\n",
    "    (Compose([StartEndMaskTransform(0.5)]), None),\n",
    "    (Compose([ReverseTimeTransform(), ConcatenateTransform()]), None),\n",
    "    (Compose([StartJitterTransform(62, 1.0), EndJitterTransform(63, 1.0)]), None),\n",
    "    (Compose([ReverseTimeTransform(), TimeDropoutTransform()]), None),\n",
    "    (Compose([ReverseTimeTransform(), TimeDropoutTransform(), ConcatenateTransform()]), None),\n",
    "    (Compose([StartJitterTransform(62, 1.0), ReverseTimeTransform(), TimeDropoutTransform()]), None),\n",
    "    (Compose([StartJitterTransform(62, 1.0), ReverseTimeTransform(), TimeDropoutTransform(), ConcatenateTransform()]), None),\n",
    "    (Compose([StartJitterTransform(62, 1.0), EndJitterTransform(63, 1.0), ReverseTimeTransform(), TimeDropoutTransform()]), None),\n",
    "    (Compose([StartJitterTransform(62, 1.0), EndJitterTransform(63, 1.0), ReverseTimeTransform(), ConcatenateTransform()]), None),\n",
    "    (Compose([StartJitterTransform(62, 1.0), EndJitterTransform(63, 1.0), ReverseTimeTransform(), TimeDropoutTransform(), ConcatenateTransform()]), None),\n",
    "    (Compose([StartJitterTransform(62, 1.0), EndJitterTransform(63, 1.0), ConcatenateTransform()]), None),\n",
    "    (Compose([StartJitterTransform(62, 1.0), EndJitterTransform(63, 1.0), TimeDropoutTransform()]), None),\n",
    "    (Compose([StartJitterTransform(62, 1.0), EndJitterTransform(63, 1.0), TimeDropoutTransform(), ConcatenateTransform()]), None),\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_transform(transforms: tuple[Compose | None, Compose | None]) -> None:\n",
    "    data_paths = [DATA_PATH / \"sat2/stage_data_proba_250hz.nc\"]\n",
    "    # data_paths = [data_path_1] # TODO: Both paths\n",
    "\n",
    "    logs_path = Path(\"../../logs/transformation_validation\")\n",
    "\n",
    "    set_global_seed(42)\n",
    "    folds = split_participants_into_folds(data_paths, N_FOLDS)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    train_transform = transforms[0]\n",
    "    testval_transform = transforms[1]\n",
    "\n",
    "    transform_name = '-'.join([tf.__class__.__name__  if tf is not None else 'None' for tf in train_transform.transforms]) if train_transform is not None else 'None'\n",
    "\n",
    "    for i_fold in range(len(folds)):\n",
    "        train_folds = deepcopy(folds)\n",
    "        test_fold = train_folds.pop(i_fold)\n",
    "        train_fold = np.concatenate(train_folds, axis=0)\n",
    "        print(f\"Fold {i_fold + 1}: test fold: {test_fold}\")\n",
    "        \n",
    "        whole_epoch = True\n",
    "        # Maybe 'accuracy'? probably not necessary\n",
    "        subset_cond = 'accuracy'\n",
    "        add_negative = True\n",
    "        norm_fn = norm_mad_zscore\n",
    "\n",
    "        run_name = f\"transform-{transform_name}_fold-{i_fold}\"\n",
    "\n",
    "        train_data = MultiXArrayProbaDataset(\n",
    "            data_paths,\n",
    "            participants_to_keep=train_fold,\n",
    "            normalization_fn=norm_fn,\n",
    "            whole_epoch=whole_epoch,\n",
    "            labels=labels,\n",
    "            subset_cond=subset_cond,\n",
    "            add_negative=add_negative,\n",
    "            transform=train_transform,\n",
    "        )\n",
    "\n",
    "        norm_vars = get_norm_vars_from_global_statistics(train_data.statistics, norm_fn)\n",
    "        class_weights = train_data.statistics[\"class_weights\"]\n",
    "        testval_data = MultiXArrayProbaDataset(\n",
    "            data_paths,\n",
    "            participants_to_keep=test_fold,\n",
    "            normalization_fn=norm_fn,\n",
    "            norm_vars=norm_vars,\n",
    "            whole_epoch=whole_epoch,\n",
    "            labels=labels,\n",
    "            subset_cond=subset_cond,\n",
    "            add_negative=add_negative,\n",
    "            transform=testval_transform\n",
    "        )\n",
    "\n",
    "        model = base_mamba()\n",
    "        test_result = train_and_test(\n",
    "            model,\n",
    "            train_data,\n",
    "            testval_data,\n",
    "            testval_data,\n",
    "            logs_path=logs_path / transform_name,\n",
    "            workers=8,\n",
    "            batch_size=64,\n",
    "            labels=labels,\n",
    "            lr=0.0005, # 0.0001\n",
    "            use_class_weights=False,\n",
    "            class_weights=class_weights,\n",
    "            whole_epoch=whole_epoch,\n",
    "            epochs=20,\n",
    "            additional_name=run_name,\n",
    "            do_test_shuffled=True\n",
    "        )\n",
    "        print(f\"Fold {i_fold + 1}, transform: {run_name}: metric_pred shape: {test_result[0]['metric_pred'].shape}\")\n",
    "        save_tensor(torch.cat((test_result[0]['metric_pred'], test_result[0]['metric_shuffled'], test_result[0]['peaks_pred'], test_result[0]['peaks_shuffled']), dim=1), logs_path / transform_name / f\"{run_name}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform(transforms[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform(transforms[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform(transforms[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform(transforms[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform(transforms[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: test fold: ['S1' 'S10']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d1331b34cdb43f98bacf89671f50c4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/137 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1303aa7bb55c40d69a4c502cfac5141c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/137 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, transform: transform-TimeMaskTransform_fold-0: metric_pred shape: torch.Size([2264, 4])\n",
      "Fold 2: test fold: ['S18' 'S15']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2408ef17c3c24f58a1df37b9e03a6a7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/136 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df2eb91ff11545e2819486852ea850fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/136 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransforms\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 57\u001b[0m, in \u001b[0;36mtest_transform\u001b[0;34m(transforms)\u001b[0m\n\u001b[1;32m     44\u001b[0m testval_data \u001b[38;5;241m=\u001b[39m MultiXArrayProbaDataset(\n\u001b[1;32m     45\u001b[0m     data_paths,\n\u001b[1;32m     46\u001b[0m     participants_to_keep\u001b[38;5;241m=\u001b[39mtest_fold,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     53\u001b[0m     transform\u001b[38;5;241m=\u001b[39mtestval_transform\n\u001b[1;32m     54\u001b[0m )\n\u001b[1;32m     56\u001b[0m model \u001b[38;5;241m=\u001b[39m base_mamba()\n\u001b[0;32m---> 57\u001b[0m test_result \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_test\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtestval_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtestval_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogs_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtransform_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0005\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 0.0001\u001b[39;49;00m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_class_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwhole_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhole_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43madditional_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_test_shuffled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     73\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi_fold\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, transform: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: metric_pred shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_result[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetric_pred\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     75\u001b[0m save_tensor(torch\u001b[38;5;241m.\u001b[39mcat((test_result[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetric_pred\u001b[39m\u001b[38;5;124m'\u001b[39m], test_result[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetric_shuffled\u001b[39m\u001b[38;5;124m'\u001b[39m], test_result[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpeaks_pred\u001b[39m\u001b[38;5;124m'\u001b[39m], test_result[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpeaks_shuffled\u001b[39m\u001b[38;5;124m'\u001b[39m]), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), logs_path \u001b[38;5;241m/\u001b[39m transform_name \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/workspace/hmp-ai/src/hmpai/pytorch/training.py:273\u001b[0m, in \u001b[0;36mtrain_and_test\u001b[0;34m(model, train_set, test_set, val_set, batch_size, epochs, workers, logs_path, additional_info, additional_name, use_class_weights, class_weights, label_smoothing, weight_decay, lr, do_spectral_decoupling, labels, seed, pretrain_fn, whole_epoch, probabilistic_labels, do_test_shuffled)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m# Train on batches in train_loader\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pretrain_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 273\u001b[0m     batch_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_spectral_decoupling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_spectral_decoupling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwhole_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhole_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    283\u001b[0m     batch_losses \u001b[38;5;241m=\u001b[39m pretrain_train(\n\u001b[1;32m    284\u001b[0m         model, train_loader, opt, loss, pretrain_fn, progress\u001b[38;5;241m=\u001b[39mtepoch\n\u001b[1;32m    285\u001b[0m     )\n",
      "File \u001b[0;32m/workspace/hmp-ai/src/hmpai/pytorch/training.py:482\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, loss_fn, progress, do_spectral_decoupling, whole_epoch)\u001b[0m\n\u001b[1;32m    478\u001b[0m loss_per_batch \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;66;03m# (Index, samples, channels), (Index, )\u001b[39;00m\n\u001b[0;32m--> 482\u001b[0m     data, labels \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m, batch[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m    484\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    486\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m model(data)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_transform(transforms[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: test fold: ['S1' 'S10']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "537da2ea6ded4b7a9961e67af712f686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/274 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../logs/transformation_validation/TimeDropoutTransform/20241028-091017_transform-TimeDropoutTransform_fold-0/checkpoint.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransforms\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 57\u001b[0m, in \u001b[0;36mtest_transform\u001b[0;34m(transforms)\u001b[0m\n\u001b[1;32m     44\u001b[0m testval_data \u001b[38;5;241m=\u001b[39m MultiXArrayProbaDataset(\n\u001b[1;32m     45\u001b[0m     data_paths,\n\u001b[1;32m     46\u001b[0m     participants_to_keep\u001b[38;5;241m=\u001b[39mtest_fold,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     53\u001b[0m     transform\u001b[38;5;241m=\u001b[39mtestval_transform\n\u001b[1;32m     54\u001b[0m )\n\u001b[1;32m     56\u001b[0m model \u001b[38;5;241m=\u001b[39m base_mamba()\n\u001b[0;32m---> 57\u001b[0m test_result \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_test\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtestval_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtestval_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogs_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtransform_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0005\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 0.0001\u001b[39;49;00m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_class_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwhole_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhole_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43madditional_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_test_shuffled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     73\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi_fold\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, transform: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: metric_pred shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_result[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetric_pred\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     75\u001b[0m save_tensor(torch\u001b[38;5;241m.\u001b[39mcat((test_result[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetric_pred\u001b[39m\u001b[38;5;124m'\u001b[39m], test_result[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetric_shuffled\u001b[39m\u001b[38;5;124m'\u001b[39m], test_result[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpeaks_pred\u001b[39m\u001b[38;5;124m'\u001b[39m], test_result[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpeaks_shuffled\u001b[39m\u001b[38;5;124m'\u001b[39m]), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), logs_path \u001b[38;5;241m/\u001b[39m transform_name \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/workspace/hmp-ai/src/hmpai/pytorch/training.py:332\u001b[0m, in \u001b[0;36mtrain_and_test\u001b[0;34m(model, train_set, test_set, val_set, batch_size, epochs, workers, logs_path, additional_info, additional_name, use_class_weights, class_weights, label_smoothing, weight_decay, lr, do_spectral_decoupling, labels, seed, pretrain_fn, whole_epoch, probabilistic_labels, do_test_shuffled)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m# Re-load best performing model\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m write_log:\n\u001b[0;32m--> 332\u001b[0m     best_checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheckpoint.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(best_checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    334\u001b[0m     opt\u001b[38;5;241m.\u001b[39mload_state_dict(best_checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer_state_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m/workspace/hmp-ai/src/hmpai/pytorch/utilities.py:50\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(path: Path) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:997\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    995\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 997\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    998\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    999\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:444\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 444\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:425\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 425\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../logs/transformation_validation/TimeDropoutTransform/20241028-091017_transform-TimeDropoutTransform_fold-0/checkpoint.pt'"
     ]
    }
   ],
   "source": [
    "test_transform(transforms[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform(transforms[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform(transforms[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform(transforms[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform(transforms[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform(transforms[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform(transforms[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform(transforms[13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform(transforms[14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform(transforms[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform(transforms[16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform(transforms[17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform(transforms[18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform(transforms[19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform(transforms[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform(transforms[21])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_path_1 = DATA_PATH / \"sat2/stage_data_proba_250hz_part1.nc\"\n",
    "data_path_2 = DATA_PATH / \"sat2/stage_data_proba_250hz_part2.nc\"\n",
    "data_paths = [data_path_1, data_path_2]\n",
    "# data_paths = [data_path_1] # TODO: Both paths\n",
    "\n",
    "logs_path = Path(\"../../logs/transformation_validation\")\n",
    "\n",
    "set_global_seed(42)\n",
    "folds = split_participants_into_folds(data_paths, N_FOLDS)\n",
    "\n",
    "results = defaultdict(list)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "for i_fold in range(len(folds)):\n",
    "    train_folds = deepcopy(folds)\n",
    "    test_fold = train_folds.pop(i_fold)\n",
    "    train_fold = np.concatenate(train_folds, axis=0)\n",
    "    print(f\"Fold {i_fold + 1}: test fold: {test_fold}\")\n",
    "\n",
    "    labels = SAT_CLASSES_ACCURACY\n",
    "    whole_epoch = True\n",
    "    # Maybe 'accuracy'? probably not necessary\n",
    "    subset_cond = 'accuracy'\n",
    "    add_negative = True\n",
    "    norm_fn = norm_mad_zscore\n",
    "\n",
    "    train_data = MultiXArrayProbaDataset(\n",
    "        data_paths,\n",
    "        participants_to_keep=train_fold,\n",
    "        normalization_fn=norm_fn,\n",
    "        whole_epoch=whole_epoch,\n",
    "        labels=labels,\n",
    "        subset_cond=subset_cond,\n",
    "        add_negative=add_negative,\n",
    "    )\n",
    "    norm_vars = get_norm_vars_from_global_statistics(train_data.statistics, norm_fn)\n",
    "    class_weights = train_data.statistics[\"class_weights\"]\n",
    "    testval_data = MultiXArrayProbaDataset(\n",
    "        data_paths,\n",
    "        participants_to_keep=test_fold,\n",
    "        normalization_fn=norm_fn,\n",
    "        norm_vars=norm_vars,\n",
    "        whole_epoch=whole_epoch,\n",
    "        labels=labels,\n",
    "        subset_cond=subset_cond,\n",
    "        add_negative=add_negative,\n",
    "    )\n",
    "\n",
    "    for i_t, (t_train, t_test) in enumerate(transforms):\n",
    "        # Set transforms\n",
    "        train_data.transform = t_train\n",
    "        testval_data.transform = t_test\n",
    "\n",
    "        model = base_mamba()\n",
    "        additional_name = 'None' if t_train is None else f\"transform-{t_train.transforms[0].__class__.__name__}_fold-{i_fold}\"\n",
    "        test_result = train_and_test(\n",
    "            model,\n",
    "            train_data,\n",
    "            testval_data,\n",
    "            testval_data,\n",
    "            logs_path=logs_path,\n",
    "            workers=8,\n",
    "            batch_size=64,\n",
    "            labels=labels,\n",
    "            lr=0.0005, # 0.0001\n",
    "            use_class_weights=False,\n",
    "            class_weights=class_weights,\n",
    "            whole_epoch=whole_epoch,\n",
    "            epochs=20,\n",
    "            additional_name=additional_name,\n",
    "            do_test_shuffled=True\n",
    "        )\n",
    "        print(f\"Fold {i_fold + 1}, transform: {str(t_train)}: EMD: {test_result[0]['EMD']}, EMD_raw: {test_result[0]['EMD_raw'].shape}\")\n",
    "        for i, result in enumerate(test_result):\n",
    "            results[i_t].append(result)\n",
    "\n",
    "for i_t, (t_train, t_test) in enumerate(transforms):\n",
    "    if isinstance(type(results[i_t][0]), dict):\n",
    "        with open(logs_path / f\"results_{str(t_train)}.json\", \"w\") as f:\n",
    "            json.dump(results[i_t], f, indent=4)\n",
    "    else:\n",
    "        for i_fold, fold in enumerate(results[i_t]):\n",
    "            tensors = fold[\"EMD_raw\"]\n",
    "            name = 'None' if t_train is None else t_train.transforms[0].__class__.__name__\n",
    "            save_tensor(tensors, logs_path / f\"results_{name}_fold_{i_fold}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
