{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a255b0c37f5499f81d57f2389facd37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='scale', max=5.0, min=0.5), FloatSlider(value=0.0, deâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gamma, norm\n",
    "from ipywidgets import interact, FloatSlider, Dropdown\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Initialize PyTorch's KL Divergence loss function\n",
    "kl_loss_fn = nn.KLDivLoss(reduction='batchmean')\n",
    "\n",
    "# Static Gamma distribution with shape parameter k=2 and scale=1\n",
    "def static_distribution(x):\n",
    "    return gamma.pdf(x, a=2, scale=1)\n",
    "\n",
    "# Define different movable distributions (Gamma, Bimodal)\n",
    "def movable_distribution(x, scale, shift, dist_type, mixing_coeff):\n",
    "    if dist_type == 'Gamma':\n",
    "        return gamma.pdf(x - shift, a=2, scale=scale)\n",
    "    elif dist_type == 'Bimodal':\n",
    "        # Bimodal distribution with the mixing coefficient applied\n",
    "        return mixing_coeff * norm.pdf(x - shift, loc=-2, scale=scale) + (1 - mixing_coeff) * norm.pdf(x - shift, loc=3, scale=scale)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported distribution type\")\n",
    "\n",
    "# Convert NumPy arrays to PyTorch tensors\n",
    "def numpy_to_torch(arr):\n",
    "    return torch.tensor(arr, dtype=torch.float32)\n",
    "\n",
    "# Function to calculate symmetrical KL divergence\n",
    "def symmetrical_kl_divergence(static_dist, movable_dist):\n",
    "    # Convert both distributions to torch tensors\n",
    "    static_dist_tensor = numpy_to_torch(static_dist)\n",
    "    movable_dist_tensor = numpy_to_torch(movable_dist)\n",
    "    \n",
    "    # Take log of both distributions\n",
    "    log_static_dist_tensor = torch.log(static_dist_tensor)\n",
    "    log_movable_dist_tensor = torch.log(movable_dist_tensor)\n",
    "    \n",
    "    # KL(static || movable)\n",
    "    kl_div_static_to_movable = kl_loss_fn(log_static_dist_tensor, movable_dist_tensor)\n",
    "    \n",
    "    # KL(movable || static)\n",
    "    kl_div_movable_to_static = kl_loss_fn(log_movable_dist_tensor, static_dist_tensor)\n",
    "    \n",
    "    # Symmetrical KL Divergence: average of both directions\n",
    "    combined_kl_div = (kl_div_static_to_movable + kl_div_movable_to_static) / 2\n",
    "    \n",
    "    return combined_kl_div\n",
    "\n",
    "# Interactive plotting function\n",
    "def plot_distributions(scale, shift, dist_type, mixing_coeff):\n",
    "    # X values range\n",
    "    x = np.linspace(0, 40, 4000)  # X-axis range for the distributions\n",
    "    \n",
    "    # Static and movable distributions\n",
    "    static_dist = static_distribution(x)\n",
    "    movable_dist = movable_distribution(x, scale, shift, dist_type, mixing_coeff)\n",
    "    \n",
    "    # Avoid numerical issues by ensuring there are no zero values in the distributions\n",
    "    static_dist = np.clip(static_dist, 1e-10, None)\n",
    "    movable_dist = np.clip(movable_dist, 1e-10, None)\n",
    "    \n",
    "    # Compute the symmetrical KL Divergence\n",
    "    combined_kl_div = symmetrical_kl_divergence(static_dist, movable_dist)\n",
    "    \n",
    "    # Plot the distributions\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(x, static_dist, label='Static Gamma Distribution (k=2, scale=1)', color='blue')\n",
    "    plt.plot(x, movable_dist, label=f'{dist_type} Distribution (scale={scale:.2f}, shift={shift:.2f}, mix={mixing_coeff:.2f})', color='red')\n",
    "    \n",
    "    # Show combined KL Divergence from PyTorch\n",
    "    plt.title(f'Symmetrical KL Divergence (PyTorch): {combined_kl_div.item():.4f}')\n",
    "    plt.legend()\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('Probability Density')\n",
    "    plt.grid(True)\n",
    "    plt.xlim((0, 40))\n",
    "    plt.show()\n",
    "\n",
    "# Dropdown to select the type of movable distribution\n",
    "distribution_dropdown = Dropdown(\n",
    "    options=['Gamma', 'Bimodal'],\n",
    "    value='Bimodal',  # Default to Bimodal so the slider makes sense\n",
    "    description='Distribution:',\n",
    ")\n",
    "\n",
    "# Sliders for the scale, shift, and mixing coefficient of the movable distribution\n",
    "interact(plot_distributions,\n",
    "         scale=FloatSlider(min=0.5, max=5.0, step=0.1, value=1),\n",
    "         shift=FloatSlider(min=-5.0, max=30.0, step=0.1, value=0),\n",
    "         dist_type=distribution_dropdown,\n",
    "         mixing_coeff=FloatSlider(min=0.0, max=1.0, step=0.01, value=0.5, description='Mix Coeff'));\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
