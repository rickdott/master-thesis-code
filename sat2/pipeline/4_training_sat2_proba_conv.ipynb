{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import netCDF4\n",
    "import xarray as xr\n",
    "from pathlib import Path\n",
    "from hmpai.pytorch.models import *\n",
    "from hmpai.training import split_data_on_participants, split_participants\n",
    "from hmpai.pytorch.training import train, validate, calculate_class_weights, train_and_test, k_fold_cross_validate, test, calculate_global_class_weights, EarlyStopper\n",
    "from hmpai.pytorch.utilities import DEVICE, set_global_seed, get_summary_str, save_model, load_model\n",
    "from hmpai.pytorch.generators import SAT1Dataset, MultiXArrayDataset, MultiXArrayProbaDataset\n",
    "from hmpai.data import SAT1_STAGES_ACCURACY, SAT_CLASSES_ACCURACY\n",
    "from hmpai.visualization import plot_confusion_matrix\n",
    "from hmpai.pytorch.normalization import *\n",
    "from torchinfo import summary\n",
    "from hmpai.utilities import print_results, CHANNELS_2D, AR_SAT1_CHANNELS\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose\n",
    "from hmpai.pytorch.transforms import *\n",
    "from hmpai.pytorch.mamba import *\n",
    "from hmpai.behaviour.sat2 import read_behavioural_info\n",
    "from hmpai.visualization import display_trial\n",
    "from hmpai.behaviour.sat2 import SAT2_SPLITS\n",
    "# from braindecode.models.eegconformer import EEGConformer\n",
    "from mne.io import read_info\n",
    "import os\n",
    "DATA_PATH = Path(os.getenv(\"DATA_PATH\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_global_seed(42)\n",
    "\n",
    "data_paths = [DATA_PATH / \"sat2/stage_data_proba_250hz.nc\"]\n",
    "\n",
    "# train_percentage=100 makes test and val 100 as well\n",
    "# splits = split_participants(data_paths, train_percentage=60)\n",
    "splits = SAT2_SPLITS\n",
    "labels = SAT_CLASSES_ACCURACY\n",
    "info_to_keep = ['event_name', 'participant', 'epochs', 'rt']\n",
    "whole_epoch = True\n",
    "# subset_cond = 'accuracy'\n",
    "subset_cond = None\n",
    "add_negative = True\n",
    "skip_samples = 0 # 62\n",
    "cut_samples = 0 # 63\n",
    "\n",
    "add_pe = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_fn = norm_mad_zscore\n",
    "train_data = MultiXArrayProbaDataset(\n",
    "    data_paths,\n",
    "    participants_to_keep=splits[0],\n",
    "    normalization_fn=norm_fn,\n",
    "    whole_epoch=whole_epoch,\n",
    "    labels=labels,\n",
    "    info_to_keep=info_to_keep,\n",
    "    subset_cond=subset_cond,\n",
    "    add_negative=add_negative,\n",
    "    # Replace with startjittertransform\n",
    "    # transform=Compose([FixedLengthCropTransform(), ReverseTimeTransform()]),\n",
    "    # transform=Compose([StartJitterTransform(62, 1.0), EndJitterTransform(63, 1.0), ReverseTimeTransform(0.1), ConcatenateTransform(0.1)]),\n",
    "    # transform=Compose([StartJitterTransform(62, 1.0), EndJitterTransform(63, 1.0), ReverseTimeTransform(0.1)]),\n",
    "    transform=Compose([StartJitterTransform(62, 1.0), EndJitterTransform(63, 1.0)]),\n",
    "    # transform=Compose([StartJitterQuadraticTransform(62, 1.0), EndJitterQuadraticTransform(63, 1.0)]),\n",
    "    # transform=Compose([ReverseTimeTransform(0.5), ConcatenateTransform(0.5)]),\n",
    "    # transform=Compose([ReverseTimeTransform(0.5), TimeDropoutTransform(), ConcatenateTransform(0.5)]),\n",
    "    # transform=Compose([StartJitterTransform(62), FixedLengthCropTransform()]),\n",
    "    skip_samples=skip_samples,\n",
    "    cut_samples=cut_samples,\n",
    "    add_pe=add_pe,\n",
    ")\n",
    "norm_vars = get_norm_vars_from_global_statistics(train_data.statistics, norm_fn)\n",
    "class_weights = train_data.statistics[\"class_weights\"]\n",
    "test_data = MultiXArrayProbaDataset(\n",
    "    data_paths,\n",
    "    participants_to_keep=splits[1],\n",
    "    normalization_fn=norm_fn,\n",
    "    norm_vars=norm_vars,\n",
    "    whole_epoch=whole_epoch,\n",
    "    labels=labels,\n",
    "    info_to_keep=info_to_keep,\n",
    "    subset_cond=subset_cond,\n",
    "    add_negative=add_negative,\n",
    "    skip_samples=skip_samples,\n",
    "    cut_samples=cut_samples,\n",
    "    add_pe=add_pe,\n",
    ")\n",
    "val_data = MultiXArrayProbaDataset(\n",
    "    data_paths,\n",
    "    participants_to_keep=splits[2],\n",
    "    normalization_fn=norm_fn,\n",
    "    norm_vars=norm_vars,\n",
    "    whole_epoch=whole_epoch,\n",
    "    labels=labels,\n",
    "    info_to_keep=info_to_keep,\n",
    "    subset_cond=subset_cond,\n",
    "    add_negative=add_negative,\n",
    "    skip_samples=skip_samples,\n",
    "    cut_samples=cut_samples,\n",
    "    add_pe=add_pe,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b9371c25c71435394cef7b12a8f3626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/642 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da5c1ac75f3a4bf4b2889a3eeae88a0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/642 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a857f5bc69b4480ca441534d76ee28d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/642 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcf02c99df174629a39d33e3fbd2d342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/642 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0505226e9634bdfad88e4fd72411457",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/642 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d4be6f5be1f49c6bfbbe8751e8d3bce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/642 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2de5b84d30a4fa88bb1cc1bc1012599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/642 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3d7de54537e4987bb14094e1c51e44b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/642 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "177979e17d30412da253dfe09b38a92a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/642 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7cb6207903846899fb615875dbb2350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/642 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_kldiv_list': [1.3765509128570557,\n",
       "   1.154708743095398,\n",
       "   1.4014230966567993,\n",
       "   0.579967200756073,\n",
       "   0.5650893449783325,\n",
       "   0.6825084686279297,\n",
       "   0.7388719320297241,\n",
       "   0.9593201875686646,\n",
       "   0.8165270090103149,\n",
       "   0.5586904287338257,\n",
       "   0.7333667874336243,\n",
       "   0.9016988277435303,\n",
       "   0.9560989141464233,\n",
       "   0.6154175996780396,\n",
       "   2.4202537536621094,\n",
       "   1.0585191249847412,\n",
       "   0.971948504447937,\n",
       "   0.671795129776001,\n",
       "   1.2099757194519043,\n",
       "   1.1889472007751465,\n",
       "   0.43571779131889343,\n",
       "   3.429997682571411,\n",
       "   0.7842995524406433,\n",
       "   0.7963616847991943,\n",
       "   0.3341410160064697,\n",
       "   0.5629545450210571,\n",
       "   0.4545060992240906,\n",
       "   0.7919501066207886,\n",
       "   0.6826044321060181,\n",
       "   0.7750592827796936,\n",
       "   1.5702720880508423,\n",
       "   0.6025288701057434,\n",
       "   0.6097281575202942,\n",
       "   0.3893619179725647,\n",
       "   1.3701198101043701,\n",
       "   0.9925764799118042,\n",
       "   1.5201363563537598,\n",
       "   0.3947899043560028,\n",
       "   1.1655327081680298,\n",
       "   0.9923205375671387,\n",
       "   0.7225768566131592,\n",
       "   0.3622663617134094,\n",
       "   1.8954575061798096,\n",
       "   2.7037241458892822,\n",
       "   0.3744649291038513,\n",
       "   1.6063350439071655,\n",
       "   0.5061384439468384,\n",
       "   0.689672589302063,\n",
       "   0.46548566222190857,\n",
       "   0.8616234660148621,\n",
       "   0.8647598624229431,\n",
       "   1.2181949615478516,\n",
       "   1.803928017616272,\n",
       "   2.177804946899414,\n",
       "   1.4278068542480469,\n",
       "   0.7083797454833984,\n",
       "   1.410754919052124,\n",
       "   1.2183692455291748,\n",
       "   1.375719666481018,\n",
       "   1.5711654424667358,\n",
       "   1.1706287860870361,\n",
       "   0.5767789483070374,\n",
       "   0.5002471804618835,\n",
       "   1.6803826093673706,\n",
       "   0.5999460816383362,\n",
       "   0.4004667401313782,\n",
       "   0.9232269525527954,\n",
       "   0.19791710376739502,\n",
       "   0.4282156229019165,\n",
       "   1.0604194402694702,\n",
       "   0.6560131907463074,\n",
       "   1.7673019170761108,\n",
       "   0.692082941532135,\n",
       "   0.7787823677062988,\n",
       "   0.3984355926513672,\n",
       "   0.7239453196525574,\n",
       "   0.683853268623352,\n",
       "   1.8215675354003906,\n",
       "   1.3425920009613037,\n",
       "   0.45808836817741394,\n",
       "   0.4959055781364441,\n",
       "   0.7611478567123413,\n",
       "   0.6395459175109863,\n",
       "   0.7646435499191284,\n",
       "   0.7745184302330017,\n",
       "   1.6629122495651245,\n",
       "   1.0347175598144531,\n",
       "   1.5451877117156982,\n",
       "   0.2735985517501831,\n",
       "   0.8766396045684814,\n",
       "   1.2484322786331177,\n",
       "   0.7539307475090027,\n",
       "   1.3538155555725098,\n",
       "   0.3205183744430542,\n",
       "   0.6342973709106445,\n",
       "   1.0845893621444702,\n",
       "   0.7451019883155823,\n",
       "   1.0836565494537354,\n",
       "   0.373871773481369,\n",
       "   0.4961289167404175,\n",
       "   0.632483959197998,\n",
       "   1.1164753437042236,\n",
       "   1.0336426496505737,\n",
       "   0.9895402193069458,\n",
       "   0.26075834035873413,\n",
       "   0.46114054322242737,\n",
       "   0.6495065689086914,\n",
       "   0.7551976442337036,\n",
       "   0.40243101119995117,\n",
       "   1.5872220993041992,\n",
       "   0.5089625120162964,\n",
       "   0.6429456472396851,\n",
       "   0.5841933488845825,\n",
       "   0.21277932822704315,\n",
       "   0.6512502431869507,\n",
       "   0.4324912428855896,\n",
       "   0.6874051094055176,\n",
       "   0.5225619077682495,\n",
       "   0.6189521551132202,\n",
       "   0.3057050108909607,\n",
       "   0.8831886649131775,\n",
       "   0.526833176612854,\n",
       "   0.735187292098999,\n",
       "   1.7429940700531006,\n",
       "   0.6487271785736084,\n",
       "   1.4235072135925293,\n",
       "   0.8260313272476196,\n",
       "   0.5766137838363647,\n",
       "   0.4716055691242218,\n",
       "   0.4624481201171875,\n",
       "   0.41912055015563965,\n",
       "   1.0137748718261719,\n",
       "   0.5370810031890869,\n",
       "   0.7730751633644104,\n",
       "   0.3796100616455078,\n",
       "   0.6720320582389832,\n",
       "   1.2851108312606812,\n",
       "   0.5998203754425049,\n",
       "   0.9093490839004517,\n",
       "   1.5630733966827393,\n",
       "   1.6285762786865234,\n",
       "   1.032254934310913,\n",
       "   1.9157774448394775,\n",
       "   1.104369878768921,\n",
       "   0.47256898880004883,\n",
       "   0.42252275347709656,\n",
       "   0.3765588700771332,\n",
       "   0.385339617729187,\n",
       "   0.16146671772003174,\n",
       "   0.46367353200912476,\n",
       "   1.0639182329177856,\n",
       "   0.756973385810852,\n",
       "   0.4451673924922943,\n",
       "   0.6622527837753296,\n",
       "   0.2105628103017807,\n",
       "   0.5220618844032288,\n",
       "   0.49102288484573364,\n",
       "   0.31369537115097046,\n",
       "   0.48303261399269104,\n",
       "   0.739663302898407,\n",
       "   0.7421894073486328,\n",
       "   0.8631609678268433,\n",
       "   0.38489454984664917,\n",
       "   0.30573660135269165,\n",
       "   0.4723179042339325,\n",
       "   0.5502891540527344,\n",
       "   0.6903200149536133,\n",
       "   0.5642073750495911,\n",
       "   1.2509675025939941,\n",
       "   0.4924832284450531,\n",
       "   0.5598305463790894,\n",
       "   0.6791494488716125,\n",
       "   0.7291702032089233,\n",
       "   1.4618921279907227,\n",
       "   0.36743950843811035,\n",
       "   0.3457341194152832,\n",
       "   1.7437443733215332,\n",
       "   0.46705031394958496,\n",
       "   0.46068650484085083,\n",
       "   0.9974384307861328,\n",
       "   0.41244733333587646,\n",
       "   0.46828019618988037,\n",
       "   0.3507465720176697,\n",
       "   0.2541934847831726,\n",
       "   0.5854851603507996,\n",
       "   0.4339560866355896,\n",
       "   0.646031379699707,\n",
       "   0.4751836955547333,\n",
       "   0.9083796739578247,\n",
       "   1.1858618259429932,\n",
       "   1.5674545764923096,\n",
       "   0.6025153398513794,\n",
       "   0.485079824924469,\n",
       "   1.7827403545379639,\n",
       "   0.8702635765075684,\n",
       "   1.3329187631607056,\n",
       "   1.680641770362854,\n",
       "   0.9515121579170227,\n",
       "   1.3832933902740479,\n",
       "   0.564301609992981,\n",
       "   2.7536110877990723,\n",
       "   0.8515174984931946,\n",
       "   1.5171213150024414,\n",
       "   1.7653789520263672,\n",
       "   1.4752018451690674,\n",
       "   0.9303145408630371,\n",
       "   1.41480553150177,\n",
       "   1.423370122909546,\n",
       "   1.231195092201233,\n",
       "   1.0054802894592285,\n",
       "   1.8059325218200684,\n",
       "   1.4623135328292847,\n",
       "   2.478320598602295,\n",
       "   2.2135138511657715,\n",
       "   3.0077993869781494,\n",
       "   2.5753657817840576,\n",
       "   0.9578433036804199,\n",
       "   1.4086071252822876,\n",
       "   2.601423740386963,\n",
       "   3.503722667694092,\n",
       "   1.506889820098877,\n",
       "   4.311755180358887,\n",
       "   2.3366103172302246,\n",
       "   0.9816316366195679,\n",
       "   1.0465577840805054,\n",
       "   2.3946542739868164,\n",
       "   0.6991699934005737,\n",
       "   2.6164603233337402,\n",
       "   3.0653929710388184,\n",
       "   1.464752197265625,\n",
       "   0.6009173393249512,\n",
       "   0.9465792179107666,\n",
       "   0.9636330604553223,\n",
       "   2.38787841796875,\n",
       "   0.9063171148300171,\n",
       "   2.103389263153076,\n",
       "   2.0704007148742676,\n",
       "   1.6849100589752197,\n",
       "   0.8103936314582825,\n",
       "   0.45937472581863403,\n",
       "   2.1323013305664062,\n",
       "   2.608884334564209,\n",
       "   1.144222378730774,\n",
       "   1.738092064857483,\n",
       "   1.4207487106323242,\n",
       "   2.118628978729248,\n",
       "   2.6554174423217773,\n",
       "   1.6620619297027588,\n",
       "   1.471588134765625,\n",
       "   1.0374088287353516,\n",
       "   1.0912284851074219,\n",
       "   1.0108342170715332,\n",
       "   0.8497291207313538,\n",
       "   0.7578751444816589,\n",
       "   0.9260355234146118,\n",
       "   0.9424528479576111,\n",
       "   1.9877889156341553,\n",
       "   4.487297534942627,\n",
       "   1.4137910604476929,\n",
       "   1.930288553237915,\n",
       "   0.759111225605011,\n",
       "   3.2645649909973145,\n",
       "   2.9531912803649902,\n",
       "   1.491912841796875,\n",
       "   2.6439480781555176,\n",
       "   5.5658698081970215,\n",
       "   3.5747995376586914,\n",
       "   2.84411883354187,\n",
       "   1.595179557800293,\n",
       "   1.4937864542007446,\n",
       "   3.614995002746582,\n",
       "   3.0338406562805176,\n",
       "   2.992302894592285,\n",
       "   4.009090423583984,\n",
       "   1.5891366004943848,\n",
       "   3.1864681243896484,\n",
       "   3.095703601837158,\n",
       "   1.8452281951904297,\n",
       "   0.6093786358833313,\n",
       "   3.0917344093322754,\n",
       "   2.1187968254089355,\n",
       "   1.4023714065551758,\n",
       "   3.138617515563965,\n",
       "   1.2390878200531006,\n",
       "   3.1397957801818848,\n",
       "   1.2144789695739746,\n",
       "   2.496778726577759,\n",
       "   1.3939235210418701,\n",
       "   1.1440212726593018,\n",
       "   2.969650983810425,\n",
       "   0.9664092063903809,\n",
       "   1.3807072639465332,\n",
       "   1.413632869720459,\n",
       "   0.6336188316345215,\n",
       "   0.7772835493087769,\n",
       "   1.7910367250442505,\n",
       "   0.5877605080604553,\n",
       "   0.7904869318008423,\n",
       "   1.039919376373291,\n",
       "   2.1675240993499756,\n",
       "   0.9358395934104919,\n",
       "   1.679215669631958,\n",
       "   2.0280113220214844,\n",
       "   2.712096691131592,\n",
       "   0.8356243371963501,\n",
       "   0.5677840709686279,\n",
       "   0.9665088653564453,\n",
       "   0.9923372268676758,\n",
       "   1.0736918449401855,\n",
       "   1.498084306716919,\n",
       "   1.3421623706817627,\n",
       "   0.7133178114891052,\n",
       "   1.0972163677215576,\n",
       "   1.2698323726654053,\n",
       "   2.0766491889953613,\n",
       "   0.8512008190155029,\n",
       "   0.6248738765716553,\n",
       "   0.924064040184021,\n",
       "   2.6673777103424072,\n",
       "   0.8015148639678955,\n",
       "   2.5881876945495605,\n",
       "   0.9877991676330566,\n",
       "   0.9807676076889038,\n",
       "   1.2580766677856445,\n",
       "   0.6358458995819092,\n",
       "   0.7227969169616699,\n",
       "   1.824249029159546,\n",
       "   0.8162118196487427,\n",
       "   1.1352667808532715,\n",
       "   0.44608455896377563,\n",
       "   1.7351322174072266,\n",
       "   1.3086628913879395,\n",
       "   1.4769349098205566,\n",
       "   0.8453639149665833,\n",
       "   1.1479613780975342,\n",
       "   2.9541282653808594,\n",
       "   2.5174708366394043,\n",
       "   0.966946005821228,\n",
       "   1.617842197418213,\n",
       "   1.4874749183654785,\n",
       "   1.6779346466064453,\n",
       "   1.6755406856536865,\n",
       "   0.62972092628479,\n",
       "   1.9914555549621582,\n",
       "   1.0117608308792114,\n",
       "   1.035622477531433,\n",
       "   1.3399949073791504,\n",
       "   0.7603542804718018,\n",
       "   1.9057809114456177,\n",
       "   1.5712592601776123,\n",
       "   2.3806257247924805,\n",
       "   1.076305627822876,\n",
       "   1.6974208354949951,\n",
       "   0.9318180680274963,\n",
       "   1.637161374092102,\n",
       "   0.8713656663894653,\n",
       "   1.7736738920211792,\n",
       "   0.650794506072998,\n",
       "   0.7435691356658936,\n",
       "   1.0925785303115845,\n",
       "   0.48921912908554077,\n",
       "   1.5139496326446533,\n",
       "   2.209285020828247,\n",
       "   0.6137192249298096,\n",
       "   1.3533451557159424,\n",
       "   1.8247981071472168,\n",
       "   1.9144721031188965,\n",
       "   2.607243776321411,\n",
       "   1.7703726291656494,\n",
       "   1.7277028560638428,\n",
       "   0.5909634232521057,\n",
       "   0.4077368378639221,\n",
       "   0.5039733648300171,\n",
       "   1.3522813320159912,\n",
       "   0.7320680618286133,\n",
       "   1.4055371284484863,\n",
       "   0.607733964920044,\n",
       "   1.4545683860778809,\n",
       "   0.5137370228767395,\n",
       "   0.9897278547286987,\n",
       "   1.6205685138702393,\n",
       "   2.0569992065429688,\n",
       "   0.6882201433181763,\n",
       "   1.2608275413513184,\n",
       "   1.0222036838531494,\n",
       "   1.3207019567489624,\n",
       "   2.399777889251709,\n",
       "   0.982133150100708,\n",
       "   0.67584627866745,\n",
       "   0.41099411249160767,\n",
       "   0.5024164915084839,\n",
       "   0.5836777091026306,\n",
       "   0.5764989852905273,\n",
       "   0.2700484097003937,\n",
       "   0.840490460395813,\n",
       "   0.6090078353881836,\n",
       "   0.23779048025608063,\n",
       "   0.6964131593704224,\n",
       "   0.677340030670166,\n",
       "   0.6656621694564819,\n",
       "   0.33410608768463135,\n",
       "   0.29378223419189453,\n",
       "   0.40923893451690674,\n",
       "   0.9620805978775024,\n",
       "   0.5149365663528442,\n",
       "   0.22991913557052612,\n",
       "   0.3868517279624939,\n",
       "   0.45487210154533386,\n",
       "   1.675363540649414,\n",
       "   0.38148337602615356,\n",
       "   0.3332771062850952,\n",
       "   0.3087169826030731,\n",
       "   0.13589993119239807,\n",
       "   1.1390995979309082,\n",
       "   0.2507226765155792,\n",
       "   0.4797762632369995,\n",
       "   0.24970179796218872,\n",
       "   0.5135732889175415,\n",
       "   0.45034801959991455,\n",
       "   0.7525112628936768,\n",
       "   0.35404106974601746,\n",
       "   0.7672405242919922,\n",
       "   0.3413902223110199,\n",
       "   0.37467119097709656,\n",
       "   0.4746919870376587,\n",
       "   0.5395635962486267,\n",
       "   0.3623768091201782,\n",
       "   1.016040563583374,\n",
       "   0.21427147090435028,\n",
       "   0.4348169267177582,\n",
       "   0.7717959880828857,\n",
       "   0.28152647614479065,\n",
       "   0.6456764340400696,\n",
       "   0.35179877281188965,\n",
       "   0.3336891233921051,\n",
       "   0.5717719793319702,\n",
       "   0.37668469548225403,\n",
       "   0.7240761518478394,\n",
       "   0.7733116745948792,\n",
       "   0.5860499143600464,\n",
       "   0.5897717475891113,\n",
       "   1.412060260772705,\n",
       "   0.43481963872909546,\n",
       "   0.6648022532463074,\n",
       "   0.6630979180335999,\n",
       "   0.4844955801963806,\n",
       "   0.42905518412590027,\n",
       "   0.24381570518016815,\n",
       "   0.5452591180801392,\n",
       "   0.40086036920547485,\n",
       "   0.420366108417511,\n",
       "   0.2750377953052521,\n",
       "   0.31566956639289856,\n",
       "   0.6214900016784668,\n",
       "   0.5161463022232056,\n",
       "   0.2664481997489929,\n",
       "   0.3762747347354889,\n",
       "   0.35702985525131226,\n",
       "   1.0336226224899292,\n",
       "   0.25578463077545166,\n",
       "   0.33359822630882263,\n",
       "   0.8262467384338379,\n",
       "   0.8502963781356812,\n",
       "   0.9369503259658813,\n",
       "   0.4843735694885254,\n",
       "   0.3230820298194885,\n",
       "   0.8349428176879883,\n",
       "   0.45250821113586426,\n",
       "   1.8570103645324707,\n",
       "   0.6147655844688416,\n",
       "   1.0590089559555054,\n",
       "   0.3610168993473053,\n",
       "   0.3660973310470581,\n",
       "   0.46300098299980164,\n",
       "   0.35076284408569336,\n",
       "   0.6065801382064819,\n",
       "   0.4422154128551483,\n",
       "   0.850229024887085,\n",
       "   0.4073989987373352,\n",
       "   0.21429088711738586,\n",
       "   0.7301019430160522,\n",
       "   0.8523345589637756,\n",
       "   1.0124080181121826,\n",
       "   1.4419705867767334,\n",
       "   1.203289270401001,\n",
       "   0.23795625567436218,\n",
       "   0.2664153575897217,\n",
       "   0.207699716091156,\n",
       "   0.1820235401391983,\n",
       "   0.27666619420051575,\n",
       "   0.8233382105827332,\n",
       "   0.48893851041793823,\n",
       "   1.008510708808899,\n",
       "   1.0236215591430664,\n",
       "   0.2503553032875061,\n",
       "   0.3290787935256958,\n",
       "   0.3064943552017212,\n",
       "   0.27980872988700867,\n",
       "   0.4045219421386719,\n",
       "   0.3201550245285034,\n",
       "   0.16969084739685059,\n",
       "   0.49120378494262695,\n",
       "   0.39378929138183594,\n",
       "   0.529105544090271,\n",
       "   0.33596184849739075,\n",
       "   0.36632704734802246,\n",
       "   0.3299574851989746,\n",
       "   0.790988564491272,\n",
       "   0.33344975113868713,\n",
       "   0.2640504240989685,\n",
       "   0.33662787079811096,\n",
       "   0.36310678720474243,\n",
       "   0.41708675026893616,\n",
       "   0.7773957848548889,\n",
       "   0.2473755180835724,\n",
       "   0.31359660625457764,\n",
       "   0.6168659925460815,\n",
       "   0.24266566336154938,\n",
       "   0.70998615026474,\n",
       "   0.3915855288505554,\n",
       "   0.6470250487327576,\n",
       "   0.6767438650131226,\n",
       "   0.2093714028596878,\n",
       "   0.4760829508304596,\n",
       "   0.3122805058956146,\n",
       "   0.44521772861480713,\n",
       "   0.637136697769165,\n",
       "   0.4356786906719208,\n",
       "   0.7574372291564941,\n",
       "   0.8880139589309692,\n",
       "   1.6375248432159424,\n",
       "   0.3801196813583374,\n",
       "   0.4234132766723633,\n",
       "   0.37014687061309814,\n",
       "   0.6736422181129456,\n",
       "   0.40359726548194885,\n",
       "   0.751400887966156,\n",
       "   0.5946186184883118,\n",
       "   0.648389458656311,\n",
       "   0.5552118420600891,\n",
       "   0.36932796239852905,\n",
       "   0.706168532371521,\n",
       "   0.4802054762840271,\n",
       "   0.3100753128528595,\n",
       "   0.2816798985004425,\n",
       "   0.5068931579589844,\n",
       "   0.7585504055023193,\n",
       "   0.6542953252792358,\n",
       "   0.47195443511009216,\n",
       "   0.44938895106315613,\n",
       "   0.8559665679931641,\n",
       "   0.37220194935798645,\n",
       "   0.2589099109172821,\n",
       "   0.3876063823699951,\n",
       "   1.3477213382720947,\n",
       "   0.562720775604248,\n",
       "   0.3950826823711395,\n",
       "   0.7346165776252747,\n",
       "   0.5927950143814087,\n",
       "   0.3625524342060089,\n",
       "   0.2431453913450241,\n",
       "   0.7629560828208923,\n",
       "   0.39547014236450195,\n",
       "   0.4010668396949768,\n",
       "   0.5478024482727051,\n",
       "   0.2220768928527832,\n",
       "   0.4018975794315338,\n",
       "   0.8662213683128357,\n",
       "   0.36610668897628784,\n",
       "   0.41183483600616455,\n",
       "   0.48826637864112854,\n",
       "   0.40673238039016724,\n",
       "   0.5889801383018494,\n",
       "   0.41186782717704773,\n",
       "   0.8241007328033447,\n",
       "   0.48544204235076904,\n",
       "   0.4023115932941437,\n",
       "   0.22177264094352722,\n",
       "   0.37736594676971436,\n",
       "   0.3634116053581238,\n",
       "   0.5175074338912964,\n",
       "   0.4580361545085907,\n",
       "   0.2054963856935501,\n",
       "   0.21676811575889587,\n",
       "   0.32300350069999695,\n",
       "   0.17894920706748962,\n",
       "   0.7558847665786743,\n",
       "   0.5111438035964966,\n",
       "   0.23096609115600586,\n",
       "   0.7065467834472656,\n",
       "   0.5471773743629456,\n",
       "   0.37419793009757996,\n",
       "   2.006072998046875,\n",
       "   1.7778193950653076,\n",
       "   1.1375830173492432,\n",
       "   1.8034368753433228,\n",
       "   1.4611742496490479,\n",
       "   2.078993558883667,\n",
       "   1.1844993829727173,\n",
       "   3.3596363067626953,\n",
       "   2.967141628265381,\n",
       "   4.746581077575684,\n",
       "   1.519154667854309,\n",
       "   4.747114658355713,\n",
       "   3.39705491065979,\n",
       "   1.7057826519012451,\n",
       "   1.1004537343978882,\n",
       "   0.939709484577179,\n",
       "   0.9369714260101318,\n",
       "   1.3416893482208252,\n",
       "   1.4872082471847534,\n",
       "   2.674891471862793,\n",
       "   1.7767277956008911,\n",
       "   1.0249347686767578,\n",
       "   1.740966796875,\n",
       "   2.016718626022339,\n",
       "   1.6940057277679443,\n",
       "   1.978132724761963,\n",
       "   0.7149335145950317,\n",
       "   0.8159116506576538,\n",
       "   0.8330711126327515,\n",
       "   2.165323257446289,\n",
       "   2.2941272258758545,\n",
       "   4.090456008911133,\n",
       "   2.2286548614501953,\n",
       "   2.5628089904785156,\n",
       "   0.8289541006088257,\n",
       "   1.9640898704528809,\n",
       "   3.3762717247009277,\n",
       "   0.5691473484039307,\n",
       "   4.75103235244751,\n",
       "   2.5799477100372314,\n",
       "   1.8828270435333252,\n",
       "   1.2459022998809814,\n",
       "   0.8401519060134888,\n",
       "   1.7920228242874146,\n",
       "   1.8033663034439087,\n",
       "   1.8478763103485107,\n",
       "   1.3098185062408447,\n",
       "   0.7217402458190918,\n",
       "   1.0283763408660889,\n",
       "   1.00429105758667,\n",
       "   2.180661916732788,\n",
       "   0.6970939040184021,\n",
       "   4.330539703369141,\n",
       "   1.7468738555908203,\n",
       "   1.6812642812728882,\n",
       "   3.951042890548706,\n",
       "   3.36655592918396,\n",
       "   2.3856191635131836,\n",
       "   3.6916122436523438,\n",
       "   2.634765863418579,\n",
       "   0.6185855865478516,\n",
       "   1.3243334293365479,\n",
       "   1.4355835914611816,\n",
       "   1.1411449909210205,\n",
       "   1.723116159439087,\n",
       "   2.085829496383667,\n",
       "   1.9476470947265625,\n",
       "   1.7275722026824951,\n",
       "   1.3191524744033813,\n",
       "   4.8690385818481445,\n",
       "   1.7470896244049072,\n",
       "   0.6465908885002136,\n",
       "   1.4024970531463623,\n",
       "   0.6763773560523987,\n",
       "   1.7125530242919922,\n",
       "   1.308591604232788,\n",
       "   1.3749899864196777,\n",
       "   0.6664700508117676,\n",
       "   1.7414395809173584,\n",
       "   1.0156786441802979,\n",
       "   3.9270362854003906,\n",
       "   1.5953118801116943,\n",
       "   1.1912955045700073,\n",
       "   2.5576722621917725,\n",
       "   0.7817191481590271,\n",
       "   1.3788011074066162,\n",
       "   0.7557322978973389,\n",
       "   0.7391853928565979,\n",
       "   1.467787742614746,\n",
       "   0.9023729562759399,\n",
       "   0.8255399465560913,\n",
       "   0.9806903600692749,\n",
       "   0.9516140818595886,\n",
       "   1.0544935464859009,\n",
       "   0.6115069389343262,\n",
       "   0.8387592434883118,\n",
       "   0.4799032211303711,\n",
       "   0.7173610925674438,\n",
       "   0.5312880277633667,\n",
       "   1.4686269760131836,\n",
       "   1.0587258338928223,\n",
       "   0.9569783806800842,\n",
       "   0.7504487037658691,\n",
       "   1.244509220123291,\n",
       "   0.7638816833496094,\n",
       "   0.6695840358734131,\n",
       "   0.9420455098152161,\n",
       "   0.4753209352493286,\n",
       "   1.023709774017334,\n",
       "   0.558998167514801,\n",
       "   0.4770854711532593,\n",
       "   0.8449705839157104,\n",
       "   2.064426898956299,\n",
       "   1.3872734308242798,\n",
       "   0.31216710805892944,\n",
       "   1.757216215133667,\n",
       "   0.6953006386756897,\n",
       "   0.597819447517395,\n",
       "   0.611972451210022,\n",
       "   0.9185936450958252,\n",
       "   0.4651983082294464,\n",
       "   0.8720828890800476,\n",
       "   1.0051326751708984,\n",
       "   1.0468904972076416,\n",
       "   0.967779815196991,\n",
       "   1.2996366024017334,\n",
       "   2.0091004371643066,\n",
       "   1.1512272357940674,\n",
       "   0.9371949434280396,\n",
       "   0.7215118408203125,\n",
       "   0.6127791404724121,\n",
       "   0.3483582139015198,\n",
       "   0.6455880999565125,\n",
       "   1.2422270774841309,\n",
       "   1.5372587442398071,\n",
       "   1.2159011363983154,\n",
       "   0.7515097260475159,\n",
       "   1.39235520362854,\n",
       "   0.47226279973983765,\n",
       "   0.883132815361023,\n",
       "   0.6790393590927124,\n",
       "   1.1228399276733398,\n",
       "   0.46650010347366333,\n",
       "   0.405638724565506,\n",
       "   3.4216983318328857,\n",
       "   0.8070067167282104,\n",
       "   0.5656217336654663,\n",
       "   0.3966086506843567,\n",
       "   1.1251550912857056,\n",
       "   0.6506977081298828,\n",
       "   1.0390841960906982,\n",
       "   1.019899606704712,\n",
       "   0.5394876599311829,\n",
       "   0.3948224186897278,\n",
       "   0.8221997022628784,\n",
       "   0.47127455472946167,\n",
       "   0.8547639846801758,\n",
       "   2.0660080909729004,\n",
       "   1.9727449417114258,\n",
       "   1.3089652061462402,\n",
       "   0.8210849761962891,\n",
       "   0.8635947108268738,\n",
       "   1.4220436811447144,\n",
       "   0.6268588304519653,\n",
       "   2.313194751739502,\n",
       "   2.104862689971924,\n",
       "   0.6321874856948853,\n",
       "   0.663898766040802,\n",
       "   1.1215523481369019,\n",
       "   0.865382730960846,\n",
       "   1.7797751426696777,\n",
       "   0.926296055316925,\n",
       "   1.373504638671875,\n",
       "   0.9203627109527588,\n",
       "   0.5723515152931213,\n",
       "   0.4458507299423218,\n",
       "   1.1617114543914795,\n",
       "   0.5753707885742188,\n",
       "   0.5402694940567017,\n",
       "   0.774982213973999,\n",
       "   0.7616063952445984,\n",
       "   1.8562153577804565,\n",
       "   0.7097004055976868,\n",
       "   1.2423651218414307,\n",
       "   0.9653056859970093,\n",
       "   1.4473261833190918,\n",
       "   0.2781364917755127,\n",
       "   1.0964269638061523,\n",
       "   0.6461213231086731,\n",
       "   0.3246334195137024,\n",
       "   0.31989040970802307,\n",
       "   0.44567257165908813,\n",
       "   0.6939445734024048,\n",
       "   0.30512306094169617,\n",
       "   0.8565922975540161,\n",
       "   1.4151334762573242,\n",
       "   1.4557141065597534,\n",
       "   1.724815011024475,\n",
       "   1.1337530612945557,\n",
       "   1.0928339958190918,\n",
       "   1.8641552925109863,\n",
       "   0.468286395072937,\n",
       "   0.895372211933136,\n",
       "   0.9645241498947144,\n",
       "   0.5149767398834229,\n",
       "   0.5258641242980957,\n",
       "   0.8068960905075073,\n",
       "   0.880652129650116,\n",
       "   0.8629807233810425,\n",
       "   0.6712912917137146,\n",
       "   1.2189451456069946,\n",
       "   0.36187177896499634,\n",
       "   0.5636368989944458,\n",
       "   0.7563398480415344,\n",
       "   0.27076455950737,\n",
       "   0.35589343309402466,\n",
       "   1.4139600992202759,\n",
       "   0.7549377679824829,\n",
       "   0.9034553170204163,\n",
       "   0.7357956767082214,\n",
       "   1.1493186950683594,\n",
       "   0.9694167971611023,\n",
       "   0.6997098922729492,\n",
       "   0.7467175722122192,\n",
       "   0.3131977915763855,\n",
       "   1.95698881149292,\n",
       "   0.9620374441146851,\n",
       "   0.6973360776901245,\n",
       "   0.47300177812576294,\n",
       "   2.4559412002563477,\n",
       "   2.100269079208374,\n",
       "   1.1950639486312866,\n",
       "   1.7411220073699951,\n",
       "   1.650834321975708,\n",
       "   0.4509662985801697,\n",
       "   0.6336863040924072,\n",
       "   0.3912632465362549,\n",
       "   0.4181605279445648,\n",
       "   0.37317776679992676,\n",
       "   0.43294596672058105,\n",
       "   1.1916426420211792,\n",
       "   1.7102338075637817,\n",
       "   0.4161076247692108,\n",
       "   0.5059297680854797,\n",
       "   1.321049690246582,\n",
       "   0.8818709850311279,\n",
       "   0.8017685413360596,\n",
       "   0.6359745264053345,\n",
       "   0.4178032875061035,\n",
       "   3.18896484375,\n",
       "   0.5284157991409302,\n",
       "   0.481876015663147,\n",
       "   0.9669709801673889,\n",
       "   0.2789672315120697,\n",
       "   0.8771518468856812,\n",
       "   3.794901132583618,\n",
       "   0.9995448589324951,\n",
       "   0.7900474071502686,\n",
       "   0.5618046522140503,\n",
       "   2.1164326667785645,\n",
       "   0.9178960919380188,\n",
       "   0.42904436588287354,\n",
       "   0.380168080329895,\n",
       "   0.870349109172821,\n",
       "   2.3291268348693848,\n",
       "   1.2673972845077515,\n",
       "   0.5513670444488525,\n",
       "   0.532931923866272,\n",
       "   1.0753936767578125,\n",
       "   0.6559921503067017,\n",
       "   0.35852062702178955,\n",
       "   0.30178192257881165,\n",
       "   0.467578262090683,\n",
       "   0.5610449314117432,\n",
       "   0.3414571285247803,\n",
       "   0.2412126362323761,\n",
       "   0.21802133321762085,\n",
       "   0.3538782000541687,\n",
       "   0.5643094182014465,\n",
       "   0.38347095251083374,\n",
       "   0.25792285799980164,\n",
       "   0.3637434244155884,\n",
       "   0.41415825486183167,\n",
       "   0.43966686725616455,\n",
       "   0.4448776841163635,\n",
       "   1.2652740478515625,\n",
       "   0.3135963976383209,\n",
       "   0.4222739338874817,\n",
       "   0.8647196888923645,\n",
       "   1.4856781959533691,\n",
       "   0.4880524277687073,\n",
       "   1.0168228149414062,\n",
       "   0.6283782124519348,\n",
       "   0.9707790613174438,\n",
       "   0.551115870475769,\n",
       "   0.4688418507575989,\n",
       "   0.6585564613342285,\n",
       "   0.21170946955680847,\n",
       "   1.0098730325698853,\n",
       "   0.5375632047653198,\n",
       "   0.31115466356277466,\n",
       "   0.20110607147216797,\n",
       "   0.5078262090682983,\n",
       "   0.9191553592681885,\n",
       "   0.412887305021286,\n",
       "   0.3265937268733978,\n",
       "   0.8613163828849792,\n",
       "   0.41566526889801025,\n",
       "   0.5424817800521851,\n",
       "   0.3197428584098816,\n",
       "   0.4832633137702942,\n",
       "   0.2748313546180725,\n",
       "   0.7555948495864868,\n",
       "   0.41079574823379517,\n",
       "   1.0691816806793213,\n",
       "   0.44959765672683716,\n",
       "   0.3921031355857849,\n",
       "   0.2947274148464203,\n",
       "   0.8098176717758179,\n",
       "   0.6593313217163086,\n",
       "   0.575939416885376,\n",
       "   0.5905017852783203,\n",
       "   0.2560625672340393,\n",
       "   0.348293662071228,\n",
       "   0.213449627161026,\n",
       "   0.47095656394958496,\n",
       "   0.22821645438671112,\n",
       "   0.8640563488006592,\n",
       "   0.3669002056121826,\n",
       "   1.3527581691741943,\n",
       "   2.845137596130371,\n",
       "   0.25994282960891724,\n",
       "   0.6309651136398315,\n",
       "   0.4395985007286072,\n",
       "   0.4465053975582123,\n",
       "   0.4303337335586548,\n",
       "   0.5243065357208252,\n",
       "   0.9352599382400513,\n",
       "   0.3920300602912903,\n",
       "   0.7060097455978394,\n",
       "   0.4132086932659149,\n",
       "   0.37089383602142334,\n",
       "   0.42491838335990906,\n",
       "   0.49160605669021606,\n",
       "   0.3537798225879669,\n",
       "   0.5049726963043213,\n",
       "   1.3801264762878418,\n",
       "   1.1468225717544556,\n",
       "   0.6911997199058533,\n",
       "   0.5702754259109497,\n",
       "   0.40538692474365234,\n",
       "   0.27644985914230347,\n",
       "   0.7149093747138977,\n",
       "   1.444769024848938,\n",
       "   0.4187501072883606,\n",
       "   0.3739640712738037,\n",
       "   0.42590343952178955,\n",
       "   0.4112556576728821,\n",
       "   0.5858496427536011,\n",
       "   0.5280447602272034,\n",
       "   0.19210262596607208,\n",
       "   0.5038020610809326,\n",
       "   0.39334940910339355,\n",
       "   0.32543855905532837,\n",
       "   0.890567421913147,\n",
       "   0.9213570356369019,\n",
       "   2.1606242656707764,\n",
       "   3.6363778114318848,\n",
       "   1.5278477668762207,\n",
       "   2.5799508094787598,\n",
       "   2.720609664916992,\n",
       "   2.496349334716797,\n",
       "   2.073298931121826,\n",
       "   2.117314338684082,\n",
       "   1.2193629741668701,\n",
       "   0.7025625705718994,\n",
       "   1.6313281059265137,\n",
       "   1.0224883556365967,\n",
       "   0.5963443517684937,\n",
       "   1.1676357984542847,\n",
       "   3.6436538696289062,\n",
       "   1.9635162353515625,\n",
       "   1.1181604862213135,\n",
       "   1.1683919429779053,\n",
       "   1.4792243242263794,\n",
       "   1.400803565979004,\n",
       "   2.306514024734497,\n",
       "   1.5272746086120605,\n",
       "   2.6207048892974854,\n",
       "   2.376121997833252,\n",
       "   1.3792189359664917,\n",
       "   2.2683565616607666,\n",
       "   1.4941397905349731,\n",
       "   1.409172534942627,\n",
       "   1.1858083009719849,\n",
       "   1.0230170488357544,\n",
       "   0.7218399047851562,\n",
       "   0.5324843525886536,\n",
       "   1.104363203048706,\n",
       "   1.23696768283844,\n",
       "   0.9108355045318604,\n",
       "   1.5667043924331665,\n",
       "   0.6941425800323486,\n",
       "   3.109272003173828,\n",
       "   1.3356854915618896,\n",
       "   0.7820432186126709,\n",
       "   1.0675709247589111,\n",
       "   0.8492509126663208,\n",
       "   ...],\n",
       "  'test_kldiv_mean': 1.0671160221099854}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# config = {'n_channels': 64,\n",
    "#   'n_classes': len(labels),\n",
    "#   'n_mamba_layers': 5,\n",
    "#   'use_pointconv_fe': True,\n",
    "#   'spatial_feature_dim': 128,\n",
    "#   'use_conv': True,\n",
    "#   'conv_kernel_sizes': [3, 9, 27],\n",
    "#   'conv_in_channels': [128, 128, 128],\n",
    "#   'conv_out_channels': [256, 256, 256],\n",
    "#   'conv_concat': True,\n",
    "#   'use_pos_enc': add_pe}\n",
    "config = {'n_channels': 64,\n",
    "  'n_classes': len(labels),\n",
    "  'n_mamba_layers': 5,\n",
    "  'use_pointconv_fe': True,\n",
    "  'spatial_feature_dim': 128,\n",
    "  'use_conv': True,\n",
    "  'conv_kernel_sizes': [3, 3, 3],\n",
    "  'conv_in_channels': [128, 256, 512],\n",
    "  'conv_out_channels': [256, 512, 768],\n",
    "  'conv_stack': True,\n",
    "  'use_pos_enc': add_pe}\n",
    "model = build_mamba(config)\n",
    "train_and_test(\n",
    "    model,\n",
    "    train_data,\n",
    "    test_data,\n",
    "    val_data,\n",
    "    logs_path=Path(\"../logs/\"),\n",
    "    workers=16,\n",
    "    batch_size=32,\n",
    "    labels=labels,\n",
    "    lr=0.0001,\n",
    "    do_spectral_decoupling=False,\n",
    "    use_class_weights=False,\n",
    "    class_weights=class_weights,\n",
    "    whole_epoch=True,\n",
    "    epochs=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in behavioural data\n",
    "behaviour_sat2 = read_behavioural_info(DATA_PATH / \"sat2/behavioural/df_full.csv\")\n",
    "test_loader_sat2 = DataLoader(\n",
    "    test_data, batch_size=128, shuffle=True, num_workers=8, pin_memory=True\n",
    ")\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in torch.randint(len(test_data), (10,)):\n",
    "    print(i)\n",
    "    display_trial(model, test_data, behaviour_sat2, i, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e19b53231533462f9167fd9f7ca50272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80644970bd5c4b2d9d519ee3530bc989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d4047d1aeff41a4aa9b57359b211ba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47558fde782b4bc0b7d6869425ea6755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e00aec2d89c43c69a217fb75e4305a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6179d3965014aa3abe8f8c7395a2bb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b021d9b183f64ec8b913de70028c015e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c2c0de143d84a8f9185913b668682cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b03255bf2e49495eb4e3ed99b78c6a2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeba51d579b945dea934d3e8baddd5ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9aba9565d5844d0b5d38125c4495f51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed5d55c57ef541028a1baeef4e939f91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f49636c53452457788d7e64c42456922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b41108067d64c6a91d0d80adc509dd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m base_mamba()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# 2 workers, ~18 b/s\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 4 workers, ~35 b/s\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 8 workers, ~48 b/s\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# 12 workers, ~48 b/s\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtrain_and_test\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogs_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../logs/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# lr=0.0005,\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# label_smoothing=0.1,\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# weight_decay=0.0001,\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_spectral_decoupling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_class_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwhole_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/hmp-ai/src/hmpai/pytorch/training.py:273\u001b[0m, in \u001b[0;36mtrain_and_test\u001b[0;34m(model, train_set, test_set, val_set, batch_size, epochs, workers, logs_path, additional_info, additional_name, use_class_weights, class_weights, label_smoothing, weight_decay, lr, do_spectral_decoupling, labels, seed, pretrain_fn, whole_epoch, probabilistic_labels, do_test_shuffled)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m# Train on batches in train_loader\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pretrain_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 273\u001b[0m     batch_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_spectral_decoupling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_spectral_decoupling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwhole_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhole_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    283\u001b[0m     batch_losses \u001b[38;5;241m=\u001b[39m pretrain_train(\n\u001b[1;32m    284\u001b[0m         model, train_loader, opt, loss, pretrain_fn, progress\u001b[38;5;241m=\u001b[39mtepoch\n\u001b[1;32m    285\u001b[0m     )\n",
      "File \u001b[0;32m/workspace/hmp-ai/src/hmpai/pytorch/training.py:498\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, loss_fn, progress, do_spectral_decoupling, whole_epoch)\u001b[0m\n\u001b[1;32m    496\u001b[0m loss \u001b[38;5;241m=\u001b[39m kldiv_loss(predictions\u001b[38;5;241m.\u001b[39mclone(), labels\u001b[38;5;241m.\u001b[39mclone())\n\u001b[1;32m    497\u001b[0m c_mse \u001b[38;5;241m=\u001b[39m cum_mse(predictions\u001b[38;5;241m.\u001b[39mclone(), labels\u001b[38;5;241m.\u001b[39mclone())\n\u001b[0;32m--> 498\u001b[0m c_mses\u001b[38;5;241m.\u001b[39mappend(\u001b[43mc_mse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    499\u001b[0m emd_val \u001b[38;5;241m=\u001b[39m emd_loss(predictions\u001b[38;5;241m.\u001b[39mclone(), labels\u001b[38;5;241m.\u001b[39mclone())\n\u001b[1;32m    500\u001b[0m emd_vals\u001b[38;5;241m.\u001b[39mappend(emd_val)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = base_mamba()\n",
    "# 2 workers, ~18 b/s\n",
    "# 4 workers, ~35 b/s\n",
    "# 8 workers, ~48 b/s\n",
    "# 12 workers, ~48 b/s\n",
    "train_and_test(\n",
    "    model,\n",
    "    train_data,\n",
    "    test_data,\n",
    "    val_data,\n",
    "    logs_path=Path(\"../logs/\"),\n",
    "    workers=8,\n",
    "    batch_size=64,\n",
    "    labels=labels,\n",
    "    lr=0.0001,\n",
    "    # lr=0.0005,\n",
    "    # label_smoothing=0.1,\n",
    "    # weight_decay=0.0001,\n",
    "    do_spectral_decoupling=False,\n",
    "    use_class_weights=False,\n",
    "    class_weights=class_weights,\n",
    "    whole_epoch=True,\n",
    "    epochs=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:306: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:1031.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "MambaModel                               [1, 638, 5]               4,883\n",
       "Linear: 1-1                            [1, 638, 128]             2,560\n",
       "Sequential: 1-2                        [1, 256, 638]             --\n",
       "    Conv1d: 2-1                       [1, 256, 638]             1,638,656\n",
       "    ReLU: 2-2                         [1, 256, 638]             --\n",
       "Sequential: 1-3                        [1, 638, 256]             --\n",
       "    MambaBlock: 2-3                   [1, 638, 256]             --\n",
       "        Mamba: 3-1                   [1, 638, 256]             437,760\n",
       "        LayerNorm: 3-2               [1, 638, 256]             512\n",
       "        Dropout: 3-3                 [1, 638, 256]             --\n",
       "    MambaBlock: 2-4                   [1, 638, 256]             --\n",
       "        Mamba: 3-4                   [1, 638, 256]             437,760\n",
       "        LayerNorm: 3-5               [1, 638, 256]             512\n",
       "        Dropout: 3-6                 [1, 638, 256]             --\n",
       "    MambaBlock: 2-5                   [1, 638, 256]             --\n",
       "        Mamba: 3-7                   [1, 638, 256]             437,760\n",
       "        LayerNorm: 3-8               [1, 638, 256]             512\n",
       "        Dropout: 3-9                 [1, 638, 256]             --\n",
       "    MambaBlock: 2-6                   [1, 638, 256]             --\n",
       "        Mamba: 3-10                  [1, 638, 256]             437,760\n",
       "        LayerNorm: 3-11              [1, 638, 256]             512\n",
       "        Dropout: 3-12                [1, 638, 256]             --\n",
       "    MambaBlock: 2-7                   [1, 638, 256]             --\n",
       "        Mamba: 3-13                  [1, 638, 256]             437,760\n",
       "        LayerNorm: 3-14              [1, 638, 256]             512\n",
       "        Dropout: 3-15                [1, 638, 256]             --\n",
       "Linear: 1-4                            [1, 638, 5]               1,285\n",
       "==========================================================================================\n",
       "Total params: 3,838,744\n",
       "Trainable params: 3,838,744\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 1.13\n",
       "==========================================================================================\n",
       "Input size (MB): 0.05\n",
       "Forward/backward pass size (MB): 29.40\n",
       "Params size (MB): 9.74\n",
       "Estimated Total Size (MB): 39.20\n",
       "=========================================================================================="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "model = MambaModel(128, 19, len(labels), \n",
    "                   5, global_pool=False, dropout=0.1)\n",
    "\n",
    "input_shape = (1, 638, 19)\n",
    "summary(model, input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SAT1GRU(64, 5)\n",
    "\n",
    "train_and_test(\n",
    "    model,\n",
    "    train_data,\n",
    "    test_data,\n",
    "    val_data,\n",
    "    logs_path=Path(\"../logs/\"),\n",
    "    workers=0,\n",
    "    batch_size=128,\n",
    "    labels=SAT_CLASSES_ACCURACY,\n",
    "    label_smoothing=0.0001,\n",
    "    weight_decay=0.01,\n",
    "    lr=0.001,\n",
    "    do_spectral_decoupling=False,\n",
    "    use_class_weights=True,\n",
    "    class_weights=class_weights,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chk_path = Path(\"../models/gru100/checkpoint.pt\")\n",
    "checkpoint = load_model(chk_path)\n",
    "\n",
    "model_kwargs = {\n",
    "    \"n_channels\": len(dataset_sat1.channels),\n",
    "    \"n_samples\": len(dataset_sat1.samples),\n",
    "    \"n_classes\": len(dataset_sat1.labels),\n",
    "}\n",
    "model = SAT1GRU(**model_kwargs)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_data, 128, shuffle=False, num_workers=4, pin_memory=True)\n",
    "results, pred, true = test(model, test_loader, None)\n",
    "pred = pred.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(\n",
    "    train_dataset_sat1, 128, shuffle=True, num_workers=4, pin_memory=True\n",
    ")\n",
    "results, pred, true = test(model, test_loader, None)\n",
    "pred = pred.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(true, pred, SAT1_STAGES_ACCURACY[1:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
