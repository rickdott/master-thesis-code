{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import netCDF4\n",
    "import xarray as xr\n",
    "from pathlib import Path\n",
    "from hmpai.pytorch.models import *\n",
    "from hmpai.training import split_data_on_participants, split_participants\n",
    "from hmpai.pytorch.training import train, validate, calculate_class_weights, train_and_test, k_fold_cross_validate, test, calculate_global_class_weights, EarlyStopper\n",
    "from hmpai.pytorch.utilities import DEVICE, set_global_seed, get_summary_str, save_model, load_model\n",
    "from hmpai.pytorch.generators import SAT1Dataset, MultiXArrayDataset, MultiXArrayProbaDataset\n",
    "from hmpai.data import SAT1_STAGES_ACCURACY, SAT_CLASSES_ACCURACY\n",
    "from hmpai.visualization import plot_confusion_matrix\n",
    "from hmpai.pytorch.normalization import *\n",
    "from torchinfo import summary\n",
    "from hmpai.utilities import print_results, CHANNELS_2D, AR_SAT1_CHANNELS\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose\n",
    "from hmpai.pytorch.transforms import *\n",
    "from hmpai.pytorch.mamba import *\n",
    "from hmpai.behaviour.sat2 import read_behavioural_info\n",
    "from hmpai.visualization import display_trial\n",
    "from hmpai.behaviour.sat2 import SAT2_SPLITS\n",
    "# from braindecode.models.eegconformer import EEGConformer\n",
    "from mne.io import read_info\n",
    "import os\n",
    "DATA_PATH = Path(os.getenv(\"DATA_PATH\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_global_seed(42)\n",
    "\n",
    "data_paths = [DATA_PATH / \"sat2/stage_data_proba_250hz_45ms.nc\"]\n",
    "\n",
    "# train_percentage=100 makes test and val 100 as well\n",
    "# splits = split_participants(data_paths, train_percentage=60)\n",
    "splits = SAT2_SPLITS\n",
    "labels = SAT_CLASSES_ACCURACY\n",
    "info_to_keep = ['event_name', 'participant', 'epochs', 'rt']\n",
    "whole_epoch = True\n",
    "subset_cond = None\n",
    "add_negative = True\n",
    "skip_samples = 0 # 62\n",
    "cut_samples = 0 # 63\n",
    "add_pe = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_fn = norm_mad_zscore\n",
    "train_data = MultiXArrayProbaDataset(\n",
    "    data_paths,\n",
    "    participants_to_keep=splits[0],\n",
    "    normalization_fn=norm_fn,\n",
    "    whole_epoch=whole_epoch,\n",
    "    labels=labels,\n",
    "    info_to_keep=info_to_keep,\n",
    "    subset_cond=subset_cond,\n",
    "    add_negative=add_negative,\n",
    "    transform=Compose([StartJitterTransform(62, 1.0), EndJitterTransform(63, 1.0)]),\n",
    "    skip_samples=skip_samples,\n",
    "    cut_samples=cut_samples,\n",
    "    add_pe=add_pe,\n",
    ")\n",
    "norm_vars = get_norm_vars_from_global_statistics(train_data.statistics, norm_fn)\n",
    "class_weights = train_data.statistics[\"class_weights\"]\n",
    "test_data = MultiXArrayProbaDataset(\n",
    "    data_paths,\n",
    "    participants_to_keep=splits[1],\n",
    "    normalization_fn=norm_fn,\n",
    "    norm_vars=norm_vars,\n",
    "    whole_epoch=whole_epoch,\n",
    "    labels=labels,\n",
    "    info_to_keep=info_to_keep,\n",
    "    subset_cond=subset_cond,\n",
    "    add_negative=add_negative,\n",
    "    skip_samples=skip_samples,\n",
    "    cut_samples=cut_samples,\n",
    "    add_pe=add_pe,\n",
    ")\n",
    "val_data = MultiXArrayProbaDataset(\n",
    "    data_paths,\n",
    "    participants_to_keep=splits[2],\n",
    "    normalization_fn=norm_fn,\n",
    "    norm_vars=norm_vars,\n",
    "    whole_epoch=whole_epoch,\n",
    "    labels=labels,\n",
    "    info_to_keep=info_to_keep,\n",
    "    subset_cond=subset_cond,\n",
    "    add_negative=add_negative,\n",
    "    skip_samples=skip_samples,\n",
    "    cut_samples=cut_samples,\n",
    "    add_pe=add_pe,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b2343c0a1634e85841c3ea01a42e350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/639 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "133ccb5dfc4c422bb3144f7954917a3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/639 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe24802e6c164897a4b5f6b707df173b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/639 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4afe61f0186e4cbf96892be12e2dd4ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/639 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e547e0e09f4465493d8f66310bb581c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/639 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cee0141cbf5c4932b7f0c520b3a523b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/639 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13e7aea8d78a4f2392f6f9d14d46c1e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/639 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d66394cf76314de58c40c3063c6db73b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/639 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0442750c9b9c45889a3cf5d14f2842d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/639 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bd7b532620a4698b1172ae8958d89c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/639 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d4b6c449cb24c02ab7767cb8cd24942",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/639 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c52c2a2fbc0f48b99302c14e4d63a59b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/639 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99ac0f0bdcb6457da4c8145f8714f163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/639 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8042d6137b5463da5e85e811d54fd8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/639 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6c533aaa417465fbe169857ae6c0908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/639 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9916c222fb344831bf9a6354d48f59e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/639 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_kldiv_list': [1.4669525623321533,\n",
       "   1.2541546821594238,\n",
       "   0.9285518527030945,\n",
       "   0.8418029546737671,\n",
       "   0.6499371528625488,\n",
       "   0.8347340822219849,\n",
       "   0.8165485858917236,\n",
       "   0.7542826533317566,\n",
       "   0.7111942768096924,\n",
       "   0.7913867831230164,\n",
       "   0.7770567536354065,\n",
       "   1.1301946640014648,\n",
       "   0.9260136485099792,\n",
       "   0.6146677732467651,\n",
       "   1.1667516231536865,\n",
       "   1.1539671421051025,\n",
       "   0.6297472715377808,\n",
       "   0.9022008180618286,\n",
       "   0.7786385416984558,\n",
       "   0.48350533843040466,\n",
       "   0.6844885349273682,\n",
       "   1.4246420860290527,\n",
       "   0.6460517644882202,\n",
       "   0.7505795955657959,\n",
       "   0.26591506600379944,\n",
       "   0.4301719069480896,\n",
       "   0.4922996163368225,\n",
       "   0.6335524320602417,\n",
       "   0.5880692005157471,\n",
       "   1.2138214111328125,\n",
       "   1.217782974243164,\n",
       "   0.8228448629379272,\n",
       "   0.5540927648544312,\n",
       "   0.3508951663970947,\n",
       "   1.1378108263015747,\n",
       "   1.1206867694854736,\n",
       "   1.2783058881759644,\n",
       "   0.5804687142372131,\n",
       "   0.8981610536575317,\n",
       "   0.5492517352104187,\n",
       "   0.8415184020996094,\n",
       "   0.5932624340057373,\n",
       "   1.3141005039215088,\n",
       "   1.4237940311431885,\n",
       "   0.4729028642177582,\n",
       "   1.5091595649719238,\n",
       "   0.5313112139701843,\n",
       "   0.44143229722976685,\n",
       "   0.8201566934585571,\n",
       "   0.7002825140953064,\n",
       "   0.7118053436279297,\n",
       "   1.4419267177581787,\n",
       "   1.2953705787658691,\n",
       "   1.3694748878479004,\n",
       "   1.1380858421325684,\n",
       "   1.0129724740982056,\n",
       "   1.6488146781921387,\n",
       "   0.9601763486862183,\n",
       "   1.3278484344482422,\n",
       "   1.2443939447402954,\n",
       "   0.9782768487930298,\n",
       "   0.7151885032653809,\n",
       "   0.7766754031181335,\n",
       "   1.2934978008270264,\n",
       "   0.4978198707103729,\n",
       "   0.4188545346260071,\n",
       "   0.685932993888855,\n",
       "   0.41667693853378296,\n",
       "   0.7057512402534485,\n",
       "   0.81255042552948,\n",
       "   0.4219704270362854,\n",
       "   0.9114773869514465,\n",
       "   0.5566115975379944,\n",
       "   0.27985358238220215,\n",
       "   0.621320366859436,\n",
       "   0.8544509410858154,\n",
       "   0.8202675580978394,\n",
       "   1.5536210536956787,\n",
       "   1.1638463735580444,\n",
       "   0.41968390345573425,\n",
       "   0.4367649555206299,\n",
       "   0.8651784658432007,\n",
       "   0.8090992569923401,\n",
       "   0.5129562616348267,\n",
       "   0.8473783731460571,\n",
       "   1.63285231590271,\n",
       "   0.716026782989502,\n",
       "   1.6227307319641113,\n",
       "   0.23045146465301514,\n",
       "   1.0194156169891357,\n",
       "   0.8287612199783325,\n",
       "   0.6541256904602051,\n",
       "   0.8621920943260193,\n",
       "   0.2355264127254486,\n",
       "   0.743837833404541,\n",
       "   0.7061485052108765,\n",
       "   0.5255693197250366,\n",
       "   0.7861878871917725,\n",
       "   0.2548123002052307,\n",
       "   0.41639944911003113,\n",
       "   0.729642391204834,\n",
       "   1.1789257526397705,\n",
       "   1.1942731142044067,\n",
       "   0.44067105650901794,\n",
       "   0.27894383668899536,\n",
       "   0.23805274069309235,\n",
       "   0.5163469314575195,\n",
       "   0.35664883255958557,\n",
       "   0.5703181028366089,\n",
       "   1.1350288391113281,\n",
       "   0.5963872671127319,\n",
       "   0.37700581550598145,\n",
       "   0.2922990620136261,\n",
       "   0.36164990067481995,\n",
       "   0.5248240828514099,\n",
       "   0.4700589179992676,\n",
       "   0.3901475667953491,\n",
       "   0.43618807196617126,\n",
       "   0.5705357193946838,\n",
       "   0.12078359723091125,\n",
       "   0.5009850263595581,\n",
       "   0.4946395456790924,\n",
       "   0.8695681691169739,\n",
       "   1.0896306037902832,\n",
       "   0.6698951721191406,\n",
       "   0.9629332423210144,\n",
       "   0.6001690626144409,\n",
       "   0.6151313781738281,\n",
       "   0.32476693391799927,\n",
       "   0.3888208270072937,\n",
       "   0.5344651937484741,\n",
       "   0.6309000253677368,\n",
       "   0.549652099609375,\n",
       "   0.5711745023727417,\n",
       "   0.27979540824890137,\n",
       "   0.5955647230148315,\n",
       "   1.029757022857666,\n",
       "   0.6739845871925354,\n",
       "   0.5902751684188843,\n",
       "   0.6796948909759521,\n",
       "   0.7624541521072388,\n",
       "   0.4900285601615906,\n",
       "   1.073601245880127,\n",
       "   0.889367401599884,\n",
       "   0.2660544514656067,\n",
       "   0.2049385905265808,\n",
       "   0.22283720970153809,\n",
       "   0.316318541765213,\n",
       "   0.3045881986618042,\n",
       "   0.44233882427215576,\n",
       "   0.8440904021263123,\n",
       "   0.6935341358184814,\n",
       "   0.3185917139053345,\n",
       "   0.5549297332763672,\n",
       "   0.31398236751556396,\n",
       "   0.4842368960380554,\n",
       "   0.4804626405239105,\n",
       "   0.28504279255867004,\n",
       "   0.2765986919403076,\n",
       "   0.28857898712158203,\n",
       "   0.6594184637069702,\n",
       "   0.8060411214828491,\n",
       "   0.6088353991508484,\n",
       "   0.5656571388244629,\n",
       "   0.3528100848197937,\n",
       "   0.4200340211391449,\n",
       "   0.2355937361717224,\n",
       "   0.19258123636245728,\n",
       "   1.329877495765686,\n",
       "   0.2644007205963135,\n",
       "   0.3795386552810669,\n",
       "   0.3627502918243408,\n",
       "   0.5054802894592285,\n",
       "   0.5534863471984863,\n",
       "   0.3814375698566437,\n",
       "   0.45750218629837036,\n",
       "   1.5254063606262207,\n",
       "   0.9694831371307373,\n",
       "   0.2645066976547241,\n",
       "   0.49787595868110657,\n",
       "   0.5546379685401917,\n",
       "   0.6251747608184814,\n",
       "   0.28119686245918274,\n",
       "   0.28569525480270386,\n",
       "   0.4382116496562958,\n",
       "   0.3215051293373108,\n",
       "   0.4448712468147278,\n",
       "   0.2372654676437378,\n",
       "   0.9124065041542053,\n",
       "   0.4639125466346741,\n",
       "   0.9308434128761292,\n",
       "   0.3915146589279175,\n",
       "   0.5404908657073975,\n",
       "   1.591249704360962,\n",
       "   0.8366701006889343,\n",
       "   0.8473089933395386,\n",
       "   0.936809778213501,\n",
       "   0.4065459370613098,\n",
       "   0.3981855809688568,\n",
       "   0.3891177475452423,\n",
       "   1.128920555114746,\n",
       "   0.44437479972839355,\n",
       "   0.7164120078086853,\n",
       "   1.1847878694534302,\n",
       "   0.6116484999656677,\n",
       "   0.3444574773311615,\n",
       "   0.7599000930786133,\n",
       "   1.66615629196167,\n",
       "   0.4569847285747528,\n",
       "   0.5259247422218323,\n",
       "   0.4898982048034668,\n",
       "   1.0412366390228271,\n",
       "   1.3673970699310303,\n",
       "   1.6012060642242432,\n",
       "   1.3441169261932373,\n",
       "   2.3530654907226562,\n",
       "   0.9541558623313904,\n",
       "   1.1716116666793823,\n",
       "   1.7462882995605469,\n",
       "   3.371220350265503,\n",
       "   0.877038836479187,\n",
       "   3.3039608001708984,\n",
       "   1.6027352809906006,\n",
       "   0.6925637722015381,\n",
       "   0.5664771795272827,\n",
       "   0.6454591751098633,\n",
       "   0.3790717124938965,\n",
       "   1.6186829805374146,\n",
       "   1.9804357290267944,\n",
       "   0.4923972487449646,\n",
       "   0.6154725551605225,\n",
       "   0.6984937191009521,\n",
       "   1.0111634731292725,\n",
       "   0.79331374168396,\n",
       "   0.43322819471359253,\n",
       "   1.5707619190216064,\n",
       "   1.3322505950927734,\n",
       "   1.0512504577636719,\n",
       "   0.6904879808425903,\n",
       "   0.6737492084503174,\n",
       "   0.8276228904724121,\n",
       "   1.0399045944213867,\n",
       "   0.7669563889503479,\n",
       "   1.415771245956421,\n",
       "   2.157313108444214,\n",
       "   0.9282132983207703,\n",
       "   1.411512851715088,\n",
       "   0.6345556974411011,\n",
       "   0.6870066523551941,\n",
       "   0.9235191345214844,\n",
       "   0.7920314073562622,\n",
       "   0.39976823329925537,\n",
       "   0.8003877401351929,\n",
       "   0.5707414150238037,\n",
       "   1.0153520107269287,\n",
       "   0.7556488513946533,\n",
       "   1.3856271505355835,\n",
       "   3.4947235584259033,\n",
       "   0.536644458770752,\n",
       "   2.2054359912872314,\n",
       "   0.41275936365127563,\n",
       "   2.1091339588165283,\n",
       "   1.8774058818817139,\n",
       "   0.7968868017196655,\n",
       "   1.6638522148132324,\n",
       "   3.6602914333343506,\n",
       "   2.1358284950256348,\n",
       "   0.7412429451942444,\n",
       "   1.1346559524536133,\n",
       "   0.557248592376709,\n",
       "   3.5094451904296875,\n",
       "   1.617347002029419,\n",
       "   2.2350871562957764,\n",
       "   2.2970640659332275,\n",
       "   1.0511202812194824,\n",
       "   2.242410659790039,\n",
       "   1.9076664447784424,\n",
       "   1.2538343667984009,\n",
       "   0.5802082419395447,\n",
       "   2.226585865020752,\n",
       "   1.7394949197769165,\n",
       "   1.3119566440582275,\n",
       "   1.8558086156845093,\n",
       "   0.8463490009307861,\n",
       "   2.0777084827423096,\n",
       "   1.1867432594299316,\n",
       "   1.084568977355957,\n",
       "   0.6985762119293213,\n",
       "   0.6170478463172913,\n",
       "   1.5660550594329834,\n",
       "   0.6061791181564331,\n",
       "   0.5028582215309143,\n",
       "   0.5677005052566528,\n",
       "   0.4311431050300598,\n",
       "   0.7234540581703186,\n",
       "   0.4054622948169708,\n",
       "   0.9763418436050415,\n",
       "   0.4305110573768616,\n",
       "   0.8632393479347229,\n",
       "   0.668986976146698,\n",
       "   0.5071804523468018,\n",
       "   0.9659989476203918,\n",
       "   0.7073765993118286,\n",
       "   1.8808825016021729,\n",
       "   1.1428043842315674,\n",
       "   0.32723769545555115,\n",
       "   0.4568935036659241,\n",
       "   0.6108313798904419,\n",
       "   0.42267972230911255,\n",
       "   0.5511898398399353,\n",
       "   1.6721245050430298,\n",
       "   0.3411099314689636,\n",
       "   0.6016978025436401,\n",
       "   0.5926885604858398,\n",
       "   1.1304683685302734,\n",
       "   0.39635005593299866,\n",
       "   0.2567133903503418,\n",
       "   0.6583806276321411,\n",
       "   1.7697314023971558,\n",
       "   0.5459737777709961,\n",
       "   1.4266244173049927,\n",
       "   0.7002845406532288,\n",
       "   0.5719885230064392,\n",
       "   0.7685328125953674,\n",
       "   0.6051913499832153,\n",
       "   0.43820106983184814,\n",
       "   1.3753161430358887,\n",
       "   0.8060241341590881,\n",
       "   0.3519314229488373,\n",
       "   0.3706819415092468,\n",
       "   0.7667111754417419,\n",
       "   0.721524715423584,\n",
       "   0.7510937452316284,\n",
       "   0.43384042382240295,\n",
       "   0.9358575344085693,\n",
       "   2.1778712272644043,\n",
       "   1.1292225122451782,\n",
       "   1.2573840618133545,\n",
       "   1.2312545776367188,\n",
       "   0.7167744636535645,\n",
       "   1.4607903957366943,\n",
       "   0.8179007172584534,\n",
       "   0.3462585508823395,\n",
       "   1.2491910457611084,\n",
       "   0.6023050546646118,\n",
       "   0.4134214520454407,\n",
       "   0.5265496969223022,\n",
       "   0.6710543632507324,\n",
       "   2.04486083984375,\n",
       "   1.0225987434387207,\n",
       "   1.7393252849578857,\n",
       "   0.9220278263092041,\n",
       "   0.9596497416496277,\n",
       "   0.9230891466140747,\n",
       "   0.6195030808448792,\n",
       "   0.825209379196167,\n",
       "   0.653801679611206,\n",
       "   0.7346835136413574,\n",
       "   0.44413265585899353,\n",
       "   0.8347154855728149,\n",
       "   0.8754538893699646,\n",
       "   0.6221168041229248,\n",
       "   1.0469164848327637,\n",
       "   0.3309091627597809,\n",
       "   1.3549593687057495,\n",
       "   1.6803991794586182,\n",
       "   0.6928429007530212,\n",
       "   1.4138370752334595,\n",
       "   0.9094369411468506,\n",
       "   1.0905652046203613,\n",
       "   0.6828539371490479,\n",
       "   0.4872651696205139,\n",
       "   0.4599859118461609,\n",
       "   0.8400864601135254,\n",
       "   0.6293691992759705,\n",
       "   0.7723691463470459,\n",
       "   0.5815995335578918,\n",
       "   0.5133976936340332,\n",
       "   0.4783983528614044,\n",
       "   0.8093687295913696,\n",
       "   0.9859926700592041,\n",
       "   1.9154971837997437,\n",
       "   0.7282213568687439,\n",
       "   0.7745402455329895,\n",
       "   0.6328198909759521,\n",
       "   0.9321917295455933,\n",
       "   2.098140239715576,\n",
       "   0.6857496500015259,\n",
       "   0.6660202145576477,\n",
       "   0.5167287588119507,\n",
       "   0.3702671527862549,\n",
       "   0.4293721318244934,\n",
       "   0.4262997806072235,\n",
       "   0.6077640056610107,\n",
       "   0.36808496713638306,\n",
       "   0.44254574179649353,\n",
       "   0.3400125801563263,\n",
       "   0.4650461673736572,\n",
       "   0.5753961801528931,\n",
       "   0.7025106549263,\n",
       "   0.26363828778266907,\n",
       "   0.17251011729240417,\n",
       "   0.3073860704898834,\n",
       "   0.5835942029953003,\n",
       "   0.4452986419200897,\n",
       "   0.18321681022644043,\n",
       "   0.4280041456222534,\n",
       "   0.759075403213501,\n",
       "   1.6176280975341797,\n",
       "   0.1794244349002838,\n",
       "   0.33245325088500977,\n",
       "   0.387040376663208,\n",
       "   0.3364766836166382,\n",
       "   0.3793511986732483,\n",
       "   0.30322980880737305,\n",
       "   0.5223869681358337,\n",
       "   0.24877548217773438,\n",
       "   0.2878619432449341,\n",
       "   0.3219926953315735,\n",
       "   0.5355315804481506,\n",
       "   0.34795135259628296,\n",
       "   0.263020396232605,\n",
       "   0.6305301189422607,\n",
       "   0.26290130615234375,\n",
       "   0.3099491000175476,\n",
       "   0.5434784889221191,\n",
       "   0.3532482981681824,\n",
       "   0.48010700941085815,\n",
       "   0.22051024436950684,\n",
       "   0.44288480281829834,\n",
       "   0.4258657991886139,\n",
       "   0.3648211359977722,\n",
       "   0.24273213744163513,\n",
       "   0.43591272830963135,\n",
       "   0.3920982778072357,\n",
       "   0.37476181983947754,\n",
       "   0.8221601247787476,\n",
       "   0.7201128005981445,\n",
       "   0.5524502396583557,\n",
       "   0.3528614640235901,\n",
       "   1.1505889892578125,\n",
       "   0.2218923568725586,\n",
       "   0.7720805406570435,\n",
       "   0.7080017328262329,\n",
       "   0.33510470390319824,\n",
       "   0.453559547662735,\n",
       "   0.21319852769374847,\n",
       "   0.19908124208450317,\n",
       "   0.26954185962677,\n",
       "   0.21561241149902344,\n",
       "   0.4663333296775818,\n",
       "   0.2595287561416626,\n",
       "   0.4314243793487549,\n",
       "   0.795006513595581,\n",
       "   0.1797027438879013,\n",
       "   0.2863699197769165,\n",
       "   0.27890902757644653,\n",
       "   0.2793874144554138,\n",
       "   0.485927551984787,\n",
       "   0.5190277695655823,\n",
       "   0.569488525390625,\n",
       "   0.6355817317962646,\n",
       "   0.9716039896011353,\n",
       "   0.3233269155025482,\n",
       "   0.3292478919029236,\n",
       "   0.29542309045791626,\n",
       "   0.4889735281467438,\n",
       "   0.5561325550079346,\n",
       "   0.31515297293663025,\n",
       "   0.9595307111740112,\n",
       "   0.3770596385002136,\n",
       "   0.3010345697402954,\n",
       "   0.33918654918670654,\n",
       "   0.248098224401474,\n",
       "   0.47806084156036377,\n",
       "   0.18367144465446472,\n",
       "   0.44038090109825134,\n",
       "   0.4473307132720947,\n",
       "   0.7837835550308228,\n",
       "   0.19758501648902893,\n",
       "   1.0129591226577759,\n",
       "   1.486693263053894,\n",
       "   1.2013177871704102,\n",
       "   0.32076913118362427,\n",
       "   0.36186403036117554,\n",
       "   0.4359515607357025,\n",
       "   0.15792739391326904,\n",
       "   0.403916597366333,\n",
       "   0.7086656093597412,\n",
       "   0.3282884955406189,\n",
       "   1.2011419534683228,\n",
       "   0.36902955174446106,\n",
       "   0.2741717994213104,\n",
       "   0.20981045067310333,\n",
       "   0.20743310451507568,\n",
       "   0.31623607873916626,\n",
       "   0.3325490951538086,\n",
       "   0.41453444957733154,\n",
       "   0.3231624662876129,\n",
       "   0.3626646399497986,\n",
       "   0.2737479507923126,\n",
       "   0.4674052894115448,\n",
       "   0.47323131561279297,\n",
       "   0.24106454849243164,\n",
       "   0.1604195088148117,\n",
       "   1.2856154441833496,\n",
       "   0.615320086479187,\n",
       "   0.40230077505111694,\n",
       "   0.5249217748641968,\n",
       "   0.35218432545661926,\n",
       "   0.3300541639328003,\n",
       "   0.4235844612121582,\n",
       "   0.2294280230998993,\n",
       "   0.13715466856956482,\n",
       "   0.3185560405254364,\n",
       "   0.27495551109313965,\n",
       "   0.37182024121284485,\n",
       "   0.3183136582374573,\n",
       "   0.6110761165618896,\n",
       "   0.7407799363136292,\n",
       "   0.34969866275787354,\n",
       "   0.4378495216369629,\n",
       "   0.2534150183200836,\n",
       "   0.33721885085105896,\n",
       "   0.36458590626716614,\n",
       "   0.36300230026245117,\n",
       "   0.24098406732082367,\n",
       "   0.4318006932735443,\n",
       "   0.40187180042266846,\n",
       "   0.2559276819229126,\n",
       "   0.9283033609390259,\n",
       "   0.28542736172676086,\n",
       "   0.4300915002822876,\n",
       "   0.22525069117546082,\n",
       "   0.57407546043396,\n",
       "   0.5196841955184937,\n",
       "   0.39268267154693604,\n",
       "   0.3319327235221863,\n",
       "   0.3205334544181824,\n",
       "   0.627273678779602,\n",
       "   0.25557923316955566,\n",
       "   0.1961393803358078,\n",
       "   0.11998742073774338,\n",
       "   0.22599589824676514,\n",
       "   0.40692412853240967,\n",
       "   0.6140433549880981,\n",
       "   0.4309957027435303,\n",
       "   0.2814376950263977,\n",
       "   0.574579119682312,\n",
       "   0.5477409958839417,\n",
       "   0.20688888430595398,\n",
       "   0.2920382618904114,\n",
       "   1.2250696420669556,\n",
       "   0.3008992373943329,\n",
       "   0.33824366331100464,\n",
       "   0.45390573143959045,\n",
       "   0.7916644811630249,\n",
       "   0.217768594622612,\n",
       "   0.17243215441703796,\n",
       "   0.4436933994293213,\n",
       "   0.35775431990623474,\n",
       "   0.35557323694229126,\n",
       "   0.5588464736938477,\n",
       "   0.2856610119342804,\n",
       "   0.27484866976737976,\n",
       "   0.23788942396640778,\n",
       "   0.3577778935432434,\n",
       "   0.33657777309417725,\n",
       "   0.2940969467163086,\n",
       "   0.2361699789762497,\n",
       "   0.31291791796684265,\n",
       "   0.18986107409000397,\n",
       "   0.520699143409729,\n",
       "   0.21352232992649078,\n",
       "   0.4842761754989624,\n",
       "   0.20652145147323608,\n",
       "   0.2090291529893875,\n",
       "   0.2582228183746338,\n",
       "   0.24973782896995544,\n",
       "   0.7613626718521118,\n",
       "   0.2178935557603836,\n",
       "   0.304024338722229,\n",
       "   0.33795231580734253,\n",
       "   0.21186822652816772,\n",
       "   0.4012637436389923,\n",
       "   0.21837642788887024,\n",
       "   0.35619163513183594,\n",
       "   0.41036900877952576,\n",
       "   0.34644442796707153,\n",
       "   0.7081419825553894,\n",
       "   1.8265612125396729,\n",
       "   1.140579104423523,\n",
       "   0.8465653657913208,\n",
       "   0.6786275506019592,\n",
       "   0.7812142968177795,\n",
       "   0.7307232022285461,\n",
       "   2.7312850952148438,\n",
       "   1.2882879972457886,\n",
       "   2.798394203186035,\n",
       "   1.3210697174072266,\n",
       "   2.7604587078094482,\n",
       "   1.7930043935775757,\n",
       "   1.4766747951507568,\n",
       "   0.4092106223106384,\n",
       "   0.5912693738937378,\n",
       "   0.7639264464378357,\n",
       "   1.0936450958251953,\n",
       "   0.8774756193161011,\n",
       "   2.1480555534362793,\n",
       "   0.9759869575500488,\n",
       "   0.7223161458969116,\n",
       "   1.162874460220337,\n",
       "   1.0537811517715454,\n",
       "   1.1458956003189087,\n",
       "   1.141045331954956,\n",
       "   0.3338378071784973,\n",
       "   0.6580933332443237,\n",
       "   0.7708878517150879,\n",
       "   1.3496429920196533,\n",
       "   2.045144557952881,\n",
       "   1.9706119298934937,\n",
       "   1.4266862869262695,\n",
       "   1.7249962091445923,\n",
       "   0.3993133306503296,\n",
       "   1.3296759128570557,\n",
       "   2.092383861541748,\n",
       "   0.5535391569137573,\n",
       "   2.0170016288757324,\n",
       "   1.8195617198944092,\n",
       "   0.7499614357948303,\n",
       "   0.24352958798408508,\n",
       "   0.6875302791595459,\n",
       "   0.6629945039749146,\n",
       "   1.0510252714157104,\n",
       "   1.4678542613983154,\n",
       "   1.0068049430847168,\n",
       "   0.6029788255691528,\n",
       "   1.1670515537261963,\n",
       "   0.5552977919578552,\n",
       "   2.2330334186553955,\n",
       "   0.5282107591629028,\n",
       "   3.2374706268310547,\n",
       "   0.9542050361633301,\n",
       "   1.2384334802627563,\n",
       "   2.185725688934326,\n",
       "   2.5551841259002686,\n",
       "   1.4673476219177246,\n",
       "   0.9564169645309448,\n",
       "   1.8801677227020264,\n",
       "   0.6252343654632568,\n",
       "   0.38249075412750244,\n",
       "   0.8276125192642212,\n",
       "   0.4806842803955078,\n",
       "   1.5324124097824097,\n",
       "   0.8748100996017456,\n",
       "   0.43048444390296936,\n",
       "   0.7315400838851929,\n",
       "   0.9815188050270081,\n",
       "   2.626068115234375,\n",
       "   0.9016157388687134,\n",
       "   0.34831351041793823,\n",
       "   0.6917317509651184,\n",
       "   0.5640439987182617,\n",
       "   0.7853790521621704,\n",
       "   0.45113274455070496,\n",
       "   1.7494158744812012,\n",
       "   0.32629138231277466,\n",
       "   1.0947848558425903,\n",
       "   0.27937206625938416,\n",
       "   1.5705320835113525,\n",
       "   1.1284799575805664,\n",
       "   1.1029045581817627,\n",
       "   0.7551172375679016,\n",
       "   0.4141818881034851,\n",
       "   0.6971302032470703,\n",
       "   0.7530577182769775,\n",
       "   0.25725314021110535,\n",
       "   0.42427754402160645,\n",
       "   0.6833947896957397,\n",
       "   0.5436581373214722,\n",
       "   0.8587156534194946,\n",
       "   0.3759848475456238,\n",
       "   0.4528281092643738,\n",
       "   0.32116225361824036,\n",
       "   0.5461994409561157,\n",
       "   1.0803543329238892,\n",
       "   0.5477975606918335,\n",
       "   1.1359975337982178,\n",
       "   0.6791518926620483,\n",
       "   0.6411863565444946,\n",
       "   0.4108163118362427,\n",
       "   0.4906229078769684,\n",
       "   0.48461058735847473,\n",
       "   0.8365956544876099,\n",
       "   0.914130687713623,\n",
       "   0.4531431794166565,\n",
       "   0.6068996787071228,\n",
       "   0.8996670842170715,\n",
       "   0.4882495105266571,\n",
       "   0.4710046947002411,\n",
       "   0.6753987073898315,\n",
       "   0.9041745066642761,\n",
       "   0.2678307890892029,\n",
       "   0.7063146829605103,\n",
       "   0.640488862991333,\n",
       "   0.34351831674575806,\n",
       "   0.7077362537384033,\n",
       "   0.6695064306259155,\n",
       "   0.9154216647148132,\n",
       "   0.3168037235736847,\n",
       "   0.5975576639175415,\n",
       "   0.5252953767776489,\n",
       "   0.9121037721633911,\n",
       "   0.5028430819511414,\n",
       "   1.0782132148742676,\n",
       "   1.1640716791152954,\n",
       "   0.39108648896217346,\n",
       "   0.4153977036476135,\n",
       "   0.5597344636917114,\n",
       "   0.5183733701705933,\n",
       "   0.3974544405937195,\n",
       "   0.29311901330947876,\n",
       "   0.7181532979011536,\n",
       "   1.4320948123931885,\n",
       "   0.5540800094604492,\n",
       "   0.42604440450668335,\n",
       "   0.5591857433319092,\n",
       "   0.34651979804039,\n",
       "   0.29137158393859863,\n",
       "   0.7691543102264404,\n",
       "   0.6003848314285278,\n",
       "   0.4576571583747864,\n",
       "   0.9209403991699219,\n",
       "   3.5639867782592773,\n",
       "   0.6974062919616699,\n",
       "   0.3719213604927063,\n",
       "   0.6871618628501892,\n",
       "   0.7320118546485901,\n",
       "   0.3730245530605316,\n",
       "   0.6398735642433167,\n",
       "   0.4384703040122986,\n",
       "   0.5413415431976318,\n",
       "   0.44991886615753174,\n",
       "   0.7579934597015381,\n",
       "   0.6503559350967407,\n",
       "   0.7596387267112732,\n",
       "   0.3322780132293701,\n",
       "   1.1675949096679688,\n",
       "   1.0567231178283691,\n",
       "   0.5288008451461792,\n",
       "   0.6971009969711304,\n",
       "   0.5464644432067871,\n",
       "   0.5414893627166748,\n",
       "   0.5686639547348022,\n",
       "   0.47564539313316345,\n",
       "   0.4543309807777405,\n",
       "   1.269941806793213,\n",
       "   0.7518369555473328,\n",
       "   0.7152935266494751,\n",
       "   0.6090661287307739,\n",
       "   0.7014592885971069,\n",
       "   0.38534581661224365,\n",
       "   0.3310515582561493,\n",
       "   0.608624279499054,\n",
       "   0.2957707643508911,\n",
       "   0.3305450677871704,\n",
       "   0.3493261933326721,\n",
       "   0.5346289873123169,\n",
       "   0.9846694469451904,\n",
       "   0.9862555861473083,\n",
       "   1.080595850944519,\n",
       "   0.4461806118488312,\n",
       "   0.6210557222366333,\n",
       "   0.8385416269302368,\n",
       "   0.3457198739051819,\n",
       "   0.4455040693283081,\n",
       "   0.8238886594772339,\n",
       "   0.7681151628494263,\n",
       "   0.584604024887085,\n",
       "   0.5224781036376953,\n",
       "   0.3136831521987915,\n",
       "   0.22421592473983765,\n",
       "   0.23758600652217865,\n",
       "   1.011333703994751,\n",
       "   0.9684519171714783,\n",
       "   0.36636513471603394,\n",
       "   1.2670243978500366,\n",
       "   0.5042672753334045,\n",
       "   0.7566843032836914,\n",
       "   1.3688132762908936,\n",
       "   0.30995166301727295,\n",
       "   0.6228418946266174,\n",
       "   0.697230339050293,\n",
       "   0.6515569686889648,\n",
       "   0.5933078527450562,\n",
       "   0.9517508745193481,\n",
       "   0.7826830744743347,\n",
       "   0.48921242356300354,\n",
       "   0.38693365454673767,\n",
       "   1.1958060264587402,\n",
       "   0.36358410120010376,\n",
       "   0.7917333841323853,\n",
       "   0.35098522901535034,\n",
       "   0.18144145607948303,\n",
       "   0.4101942181587219,\n",
       "   0.8502607941627502,\n",
       "   0.256344199180603,\n",
       "   0.44126179814338684,\n",
       "   0.8887683153152466,\n",
       "   0.8901137113571167,\n",
       "   1.142761468887329,\n",
       "   0.8035447597503662,\n",
       "   1.0010013580322266,\n",
       "   0.3609806299209595,\n",
       "   1.778395652770996,\n",
       "   1.154458999633789,\n",
       "   0.8997734785079956,\n",
       "   0.8402309417724609,\n",
       "   2.7620630264282227,\n",
       "   1.3286739587783813,\n",
       "   1.276834487915039,\n",
       "   1.560967206954956,\n",
       "   1.1366922855377197,\n",
       "   0.4199312925338745,\n",
       "   0.46264761686325073,\n",
       "   0.2026318609714508,\n",
       "   0.42773759365081787,\n",
       "   0.3821170926094055,\n",
       "   0.32084110379219055,\n",
       "   0.9978570342063904,\n",
       "   1.126833438873291,\n",
       "   0.8981761336326599,\n",
       "   0.5013388395309448,\n",
       "   1.0225496292114258,\n",
       "   0.7456750869750977,\n",
       "   0.28454747796058655,\n",
       "   0.4302666485309601,\n",
       "   0.2941230237483978,\n",
       "   1.2988349199295044,\n",
       "   0.5021822452545166,\n",
       "   0.3352629542350769,\n",
       "   1.023756980895996,\n",
       "   0.5532667636871338,\n",
       "   0.6091344356536865,\n",
       "   1.6950221061706543,\n",
       "   1.0802353620529175,\n",
       "   0.636864185333252,\n",
       "   0.4953446090221405,\n",
       "   1.8663055896759033,\n",
       "   1.0376455783843994,\n",
       "   0.5799217820167542,\n",
       "   0.6139916181564331,\n",
       "   0.41700831055641174,\n",
       "   1.3317029476165771,\n",
       "   1.444148302078247,\n",
       "   0.5165070295333862,\n",
       "   0.28767770528793335,\n",
       "   0.6856136918067932,\n",
       "   0.34581458568573,\n",
       "   0.32452625036239624,\n",
       "   0.552157998085022,\n",
       "   0.5059226751327515,\n",
       "   0.4237157106399536,\n",
       "   0.23507973551750183,\n",
       "   0.20205417275428772,\n",
       "   0.1335126906633377,\n",
       "   0.2709539830684662,\n",
       "   0.37820935249328613,\n",
       "   0.6642147898674011,\n",
       "   0.26337936520576477,\n",
       "   0.2992594242095947,\n",
       "   0.36526742577552795,\n",
       "   0.257526159286499,\n",
       "   0.4890826344490051,\n",
       "   1.19120454788208,\n",
       "   0.2547174096107483,\n",
       "   0.39173832535743713,\n",
       "   0.38588136434555054,\n",
       "   1.170830488204956,\n",
       "   0.487021803855896,\n",
       "   0.663417398929596,\n",
       "   0.3188928961753845,\n",
       "   1.8130742311477661,\n",
       "   0.43402212858200073,\n",
       "   0.4359666705131531,\n",
       "   0.5593690872192383,\n",
       "   0.21550336480140686,\n",
       "   0.8575537204742432,\n",
       "   0.3523237705230713,\n",
       "   0.40621447563171387,\n",
       "   0.20052853226661682,\n",
       "   0.3556784391403198,\n",
       "   0.2634742856025696,\n",
       "   0.5252967476844788,\n",
       "   0.19514988362789154,\n",
       "   0.6678276658058167,\n",
       "   0.30144134163856506,\n",
       "   0.2613699436187744,\n",
       "   0.2258739173412323,\n",
       "   0.27108824253082275,\n",
       "   0.27876585721969604,\n",
       "   1.0185837745666504,\n",
       "   0.2330567091703415,\n",
       "   1.0233474969863892,\n",
       "   0.4719811975955963,\n",
       "   0.27025899291038513,\n",
       "   0.36719802021980286,\n",
       "   0.3823028802871704,\n",
       "   0.32053959369659424,\n",
       "   0.33040153980255127,\n",
       "   0.5838671922683716,\n",
       "   0.14257466793060303,\n",
       "   0.30732110142707825,\n",
       "   0.36594998836517334,\n",
       "   0.17660528421401978,\n",
       "   0.4007877707481384,\n",
       "   0.25471657514572144,\n",
       "   0.7581573724746704,\n",
       "   0.41360053420066833,\n",
       "   0.2628166079521179,\n",
       "   0.24722079932689667,\n",
       "   0.2920703589916229,\n",
       "   0.5384933948516846,\n",
       "   0.5473532676696777,\n",
       "   0.6081451177597046,\n",
       "   0.41765910387039185,\n",
       "   0.5316466093063354,\n",
       "   0.27397409081459045,\n",
       "   0.38480404019355774,\n",
       "   0.3448757529258728,\n",
       "   0.6144291162490845,\n",
       "   0.28373458981513977,\n",
       "   0.35603398084640503,\n",
       "   0.5879419445991516,\n",
       "   1.2315056324005127,\n",
       "   0.5725221633911133,\n",
       "   0.4408667981624603,\n",
       "   0.3422771096229553,\n",
       "   0.3928368389606476,\n",
       "   0.8536288738250732,\n",
       "   1.0401968955993652,\n",
       "   0.3615325391292572,\n",
       "   0.5264334678649902,\n",
       "   0.8252021074295044,\n",
       "   0.30185937881469727,\n",
       "   0.6129919290542603,\n",
       "   0.701652467250824,\n",
       "   0.22281718254089355,\n",
       "   0.3895324766635895,\n",
       "   0.5291453003883362,\n",
       "   0.23572048544883728,\n",
       "   0.3962356448173523,\n",
       "   0.6965961456298828,\n",
       "   0.5366195440292358,\n",
       "   2.9245691299438477,\n",
       "   1.0288020372390747,\n",
       "   2.6150712966918945,\n",
       "   0.9958862662315369,\n",
       "   2.094459295272827,\n",
       "   2.0624494552612305,\n",
       "   1.6273572444915771,\n",
       "   0.7901880741119385,\n",
       "   0.4446450471878052,\n",
       "   1.9185562133789062,\n",
       "   0.5289124846458435,\n",
       "   0.36014288663864136,\n",
       "   0.5298370718955994,\n",
       "   1.0541980266571045,\n",
       "   1.9294183254241943,\n",
       "   1.704113483428955,\n",
       "   0.5913063287734985,\n",
       "   0.8970518112182617,\n",
       "   1.213776707649231,\n",
       "   1.4128587245941162,\n",
       "   0.9159278273582458,\n",
       "   1.588835597038269,\n",
       "   1.6325938701629639,\n",
       "   0.9226025342941284,\n",
       "   1.4305965900421143,\n",
       "   1.0387705564498901,\n",
       "   1.2594681978225708,\n",
       "   0.7522997856140137,\n",
       "   2.1592702865600586,\n",
       "   0.34284618496894836,\n",
       "   0.6685896515846252,\n",
       "   0.6177213788032532,\n",
       "   1.6748874187469482,\n",
       "   0.8877197504043579,\n",
       "   0.40550485253334045,\n",
       "   0.3999953269958496,\n",
       "   2.6165590286254883,\n",
       "   0.6771256327629089,\n",
       "   0.37676161527633667,\n",
       "   0.5652220249176025,\n",
       "   0.41484224796295166,\n",
       "   0.8117703795433044,\n",
       "   1.8739657402038574,\n",
       "   1.0626940727233887,\n",
       "   0.9105132818222046,\n",
       "   0.8658190369606018,\n",
       "   ...],\n",
       "  'test_kldiv_mean': 0.7902029156684875}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\n",
    "    \"n_channels\": 64,\n",
    "    \"n_classes\": len(labels),\n",
    "    \"n_mamba_layers\": 5,\n",
    "    \"use_pointconv_fe\": True,\n",
    "    \"spatial_feature_dim\": 128,\n",
    "    \"use_conv\": True,\n",
    "    \"conv_kernel_sizes\": [3, 9, 27],\n",
    "    \"conv_in_channels\": [128, 128, 128],\n",
    "    \"conv_out_channels\": [256, 256, 256],\n",
    "    \"conv_concat\": True,\n",
    "    \"use_pos_enc\": True,\n",
    "}\n",
    "model = build_mamba(config)\n",
    "train_and_test(\n",
    "    model,\n",
    "    train_data,\n",
    "    test_data,\n",
    "    val_data,\n",
    "    logs_path=Path(\"../logs/\"),\n",
    "    workers=12,\n",
    "    batch_size=32,\n",
    "    labels=labels,\n",
    "    lr=0.00005,\n",
    "    do_spectral_decoupling=False,\n",
    "    use_class_weights=False,\n",
    "    class_weights=class_weights,\n",
    "    whole_epoch=True,\n",
    "    epochs=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in behavioural data\n",
    "behaviour_sat2 = read_behavioural_info(DATA_PATH / \"sat2/behavioural/df_full.csv\")\n",
    "test_loader_sat2 = DataLoader(\n",
    "    test_data, batch_size=128, shuffle=True, num_workers=8, pin_memory=True\n",
    ")\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in torch.randint(len(test_data), (10,)):\n",
    "    print(i)\n",
    "    display_trial(model, test_data, behaviour_sat2, i, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e19b53231533462f9167fd9f7ca50272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80644970bd5c4b2d9d519ee3530bc989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d4047d1aeff41a4aa9b57359b211ba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47558fde782b4bc0b7d6869425ea6755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e00aec2d89c43c69a217fb75e4305a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6179d3965014aa3abe8f8c7395a2bb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b021d9b183f64ec8b913de70028c015e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c2c0de143d84a8f9185913b668682cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b03255bf2e49495eb4e3ed99b78c6a2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeba51d579b945dea934d3e8baddd5ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9aba9565d5844d0b5d38125c4495f51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed5d55c57ef541028a1baeef4e939f91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f49636c53452457788d7e64c42456922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b41108067d64c6a91d0d80adc509dd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m base_mamba()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# 2 workers, ~18 b/s\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 4 workers, ~35 b/s\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 8 workers, ~48 b/s\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# 12 workers, ~48 b/s\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtrain_and_test\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogs_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../logs/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# lr=0.0005,\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# label_smoothing=0.1,\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# weight_decay=0.0001,\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_spectral_decoupling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_class_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwhole_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/hmp-ai/src/hmpai/pytorch/training.py:273\u001b[0m, in \u001b[0;36mtrain_and_test\u001b[0;34m(model, train_set, test_set, val_set, batch_size, epochs, workers, logs_path, additional_info, additional_name, use_class_weights, class_weights, label_smoothing, weight_decay, lr, do_spectral_decoupling, labels, seed, pretrain_fn, whole_epoch, probabilistic_labels, do_test_shuffled)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m# Train on batches in train_loader\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pretrain_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 273\u001b[0m     batch_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_spectral_decoupling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_spectral_decoupling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwhole_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhole_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    283\u001b[0m     batch_losses \u001b[38;5;241m=\u001b[39m pretrain_train(\n\u001b[1;32m    284\u001b[0m         model, train_loader, opt, loss, pretrain_fn, progress\u001b[38;5;241m=\u001b[39mtepoch\n\u001b[1;32m    285\u001b[0m     )\n",
      "File \u001b[0;32m/workspace/hmp-ai/src/hmpai/pytorch/training.py:498\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, loss_fn, progress, do_spectral_decoupling, whole_epoch)\u001b[0m\n\u001b[1;32m    496\u001b[0m loss \u001b[38;5;241m=\u001b[39m kldiv_loss(predictions\u001b[38;5;241m.\u001b[39mclone(), labels\u001b[38;5;241m.\u001b[39mclone())\n\u001b[1;32m    497\u001b[0m c_mse \u001b[38;5;241m=\u001b[39m cum_mse(predictions\u001b[38;5;241m.\u001b[39mclone(), labels\u001b[38;5;241m.\u001b[39mclone())\n\u001b[0;32m--> 498\u001b[0m c_mses\u001b[38;5;241m.\u001b[39mappend(\u001b[43mc_mse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    499\u001b[0m emd_val \u001b[38;5;241m=\u001b[39m emd_loss(predictions\u001b[38;5;241m.\u001b[39mclone(), labels\u001b[38;5;241m.\u001b[39mclone())\n\u001b[1;32m    500\u001b[0m emd_vals\u001b[38;5;241m.\u001b[39mappend(emd_val)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = base_mamba()\n",
    "# 2 workers, ~18 b/s\n",
    "# 4 workers, ~35 b/s\n",
    "# 8 workers, ~48 b/s\n",
    "# 12 workers, ~48 b/s\n",
    "train_and_test(\n",
    "    model,\n",
    "    train_data,\n",
    "    test_data,\n",
    "    val_data,\n",
    "    logs_path=Path(\"../logs/\"),\n",
    "    workers=8,\n",
    "    batch_size=64,\n",
    "    labels=labels,\n",
    "    lr=0.0001,\n",
    "    # lr=0.0005,\n",
    "    # label_smoothing=0.1,\n",
    "    # weight_decay=0.0001,\n",
    "    do_spectral_decoupling=False,\n",
    "    use_class_weights=False,\n",
    "    class_weights=class_weights,\n",
    "    whole_epoch=True,\n",
    "    epochs=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:306: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:1031.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "MambaModel                               [1, 638, 5]               4,883\n",
       "Linear: 1-1                            [1, 638, 128]             2,560\n",
       "Sequential: 1-2                        [1, 256, 638]             --\n",
       "    Conv1d: 2-1                       [1, 256, 638]             1,638,656\n",
       "    ReLU: 2-2                         [1, 256, 638]             --\n",
       "Sequential: 1-3                        [1, 638, 256]             --\n",
       "    MambaBlock: 2-3                   [1, 638, 256]             --\n",
       "        Mamba: 3-1                   [1, 638, 256]             437,760\n",
       "        LayerNorm: 3-2               [1, 638, 256]             512\n",
       "        Dropout: 3-3                 [1, 638, 256]             --\n",
       "    MambaBlock: 2-4                   [1, 638, 256]             --\n",
       "        Mamba: 3-4                   [1, 638, 256]             437,760\n",
       "        LayerNorm: 3-5               [1, 638, 256]             512\n",
       "        Dropout: 3-6                 [1, 638, 256]             --\n",
       "    MambaBlock: 2-5                   [1, 638, 256]             --\n",
       "        Mamba: 3-7                   [1, 638, 256]             437,760\n",
       "        LayerNorm: 3-8               [1, 638, 256]             512\n",
       "        Dropout: 3-9                 [1, 638, 256]             --\n",
       "    MambaBlock: 2-6                   [1, 638, 256]             --\n",
       "        Mamba: 3-10                  [1, 638, 256]             437,760\n",
       "        LayerNorm: 3-11              [1, 638, 256]             512\n",
       "        Dropout: 3-12                [1, 638, 256]             --\n",
       "    MambaBlock: 2-7                   [1, 638, 256]             --\n",
       "        Mamba: 3-13                  [1, 638, 256]             437,760\n",
       "        LayerNorm: 3-14              [1, 638, 256]             512\n",
       "        Dropout: 3-15                [1, 638, 256]             --\n",
       "Linear: 1-4                            [1, 638, 5]               1,285\n",
       "==========================================================================================\n",
       "Total params: 3,838,744\n",
       "Trainable params: 3,838,744\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 1.13\n",
       "==========================================================================================\n",
       "Input size (MB): 0.05\n",
       "Forward/backward pass size (MB): 29.40\n",
       "Params size (MB): 9.74\n",
       "Estimated Total Size (MB): 39.20\n",
       "=========================================================================================="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "model = MambaModel(128, 19, len(labels), \n",
    "                   5, global_pool=False, dropout=0.1)\n",
    "\n",
    "input_shape = (1, 638, 19)\n",
    "summary(model, input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SAT1GRU(64, 5)\n",
    "\n",
    "train_and_test(\n",
    "    model,\n",
    "    train_data,\n",
    "    test_data,\n",
    "    val_data,\n",
    "    logs_path=Path(\"../logs/\"),\n",
    "    workers=0,\n",
    "    batch_size=128,\n",
    "    labels=SAT_CLASSES_ACCURACY,\n",
    "    label_smoothing=0.0001,\n",
    "    weight_decay=0.01,\n",
    "    lr=0.001,\n",
    "    do_spectral_decoupling=False,\n",
    "    use_class_weights=True,\n",
    "    class_weights=class_weights,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chk_path = Path(\"../models/gru100/checkpoint.pt\")\n",
    "checkpoint = load_model(chk_path)\n",
    "\n",
    "model_kwargs = {\n",
    "    \"n_channels\": len(dataset_sat1.channels),\n",
    "    \"n_samples\": len(dataset_sat1.samples),\n",
    "    \"n_classes\": len(dataset_sat1.labels),\n",
    "}\n",
    "model = SAT1GRU(**model_kwargs)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_data, 128, shuffle=False, num_workers=4, pin_memory=True)\n",
    "results, pred, true = test(model, test_loader, None)\n",
    "pred = pred.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(\n",
    "    train_dataset_sat1, 128, shuffle=True, num_workers=4, pin_memory=True\n",
    ")\n",
    "results, pred, true = test(model, test_loader, None)\n",
    "pred = pred.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(true, pred, SAT1_STAGES_ACCURACY[1:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
